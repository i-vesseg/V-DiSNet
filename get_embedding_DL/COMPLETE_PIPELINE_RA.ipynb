{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from utils import set_random_seed, mk_dir\n",
    "from importlib import import_module\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion() \n",
    "\n",
    "def save_img(tensor, name, norm, n_rows=16, scale_each=False):\n",
    "    save_image(tensor, name, nrow=n_rows, padding=5, normalize=norm, pad_value=1, scale_each=scale_each)\n",
    "    \n",
    "def plot_10_patches(img_np, true_vessel_np=False, indexes=None):\n",
    "    \n",
    "    random_indexes = np.random.randint(0, img_np.shape[0], size=5) if indexes is None else indexes\n",
    "\n",
    "    n = len(random_indexes)\n",
    "    \n",
    "    fig, ax = plt.subplots(n, 2, figsize=(6, 3*n))\n",
    "    \n",
    "    for i, index in enumerate(random_indexes):\n",
    "        \n",
    "        if index == img_np.shape[0]:\n",
    "            index -= 1\n",
    "        if index == img_np.shape[0]-1:\n",
    "            index -= 2\n",
    "            \n",
    "        #print(f\"Shape: {img_np[index,:,:].shape}, Max: {img_np[index,:,:].max()}, Min: {img_np[index,:,:].min()}\")\n",
    "        ax[i, 0].set_title(f'True Vessel Slice {index}')\n",
    "        ax[i, 0].imshow(img_np[index,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        ax[i,0].axis('off')\n",
    "        \n",
    "        if true_vessel_np is not False:\n",
    "            print(f\"Shape: {true_vessel_np[index,:,:].shape}, Max: {true_vessel_np[index,:,:].max()}, Min: {true_vessel_np[index,:,:].min()}\")\n",
    "            ax[i, 1].imshow(true_vessel_np[index,:,:], cmap='gray')\n",
    "            ax[i, 1].set_title(f'Image Slice {index}')\n",
    "        else:\n",
    "            ax[i, 1].set_title(f'Image Slice {index+1}')\n",
    "            ax[i, 1].imshow(img_np[index+1,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        ax[i,1].axis('off')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_n_patches_overlap(img_np, true_vessel_np=False, indexes=None, selected_class=None, add_title='', m=5, alpha=0.5, save_dir='clusters_imgs'):\n",
    "    \n",
    "    save_dir = os.path.join(save_embeddings_path,save_dir)\n",
    "    mk_dir(save_dir)\n",
    "    plt.ioff()\n",
    "    n = len(indexes) if indexes is not None else 5\n",
    "    # m is the number of images to plot in each row (2m is the number of columns)\n",
    "    n_rows = int(np.ceil(n/m))\n",
    "    n_cols = m\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, index in enumerate(indexes):\n",
    "        ax[i].set_title(f'Overlay for Slice {index}')\n",
    "        masked_image = np.ma.masked_where(img_np[index,:,:] == 0, true_vessel_np[index,:,:])\n",
    "        # Overlay the red image on top of true_vessel_np[index,:,:]\n",
    "        ax[i].imshow(true_vessel_np[index,:,:], cmap='gray', interpolation='none')\n",
    "        ax[i].imshow(masked_image, cmap='Reds', alpha=alpha)\n",
    "        \n",
    "        ax[i].axis('off')\n",
    "    \n",
    "    try:\n",
    "        plt.tight_layout()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if add_title!='':\n",
    "        print(f\"Save Fig to {save_dir}\")\n",
    "        plt.savefig(os.path.join(save_dir,f'{add_title}_class_{selected_class}_patches.png'))\n",
    "        plt.close(fig)\n",
    "        \n",
    "    else:\n",
    "        print(\"Plotting..\")\n",
    "        plt.show()\n",
    "    plt.ion()\n",
    "\n",
    "    \n",
    "def plot_n_patches(img_np, true_vessel_np=False, indexes=None, selected_class=None, add_title='', m=5):\n",
    "    plt.ioff()\n",
    "    n = len(indexes) if indexes is not None else 5\n",
    "    if n > 1000:\n",
    "        print(f'WARNING: YOU ARE TRYING TO PLOT {n} images')\n",
    "    # m is the number of images to plot in each row (2m is the number of columns)\n",
    "    n_rows = int(np.ceil(n/m))\n",
    "    n_cols = 2*m\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, index in enumerate(indexes):\n",
    "        \n",
    "        if index == img_np.shape[0]:\n",
    "            index -= 1\n",
    "        if index == img_np.shape[0]-1:\n",
    "            index -= 2\n",
    "            \n",
    "        ax[2*i].set_title(f'Image Slice {index}')\n",
    "        ax[2*i].imshow(img_np[index,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        ax[2*i].axis('off')\n",
    "        print(f\"Shape: {img_np[index,:,:].shape}, Max: {img_np[index,:,:].max()}, Min: {img_np[index,:,:].min()}\")\n",
    "        if true_vessel_np is not False:\n",
    "            ax[2*i + 1].imshow(true_vessel_np[index,:,:], cmap='gray')\n",
    "            ax[2*i + 1].set_title(f'True Vessel Slice {index}')\n",
    "            \n",
    "        else:\n",
    "            ax[2*i + 1].set_title(f'Image Slice {index+1}')\n",
    "            ax[2*i + 1].imshow(img_np[index+1,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        ax[2*i+1].axis('off')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if add_title!='':\n",
    "        print(\"Save Fig\")\n",
    "        plt.savefig(f'{add_title}_class_{selected_class}_patches.png')\n",
    "        plt.close(fig)\n",
    "        \n",
    "    else:\n",
    "        print(\"Plotting..\")\n",
    "        plt.show()\n",
    "    plt.ion()\n",
    "    \n",
    "def reshape_to_square(vector):\n",
    "    # Calculate the nearest square number greater than or equal to the length of the vector\n",
    "    n = int(np.ceil(np.sqrt(len(vector))))\n",
    "    \n",
    "    # Calculate the number of elements to pad with zeros\n",
    "    num_zeros = n*n - len(vector)\n",
    "    \n",
    "    # Pad the vector with zeros if necessary\n",
    "    vector_padded = np.pad(vector, (0, num_zeros), mode='constant')\n",
    "    \n",
    "    # Reshape the padded vector into a square matrix\n",
    "    square_matrix = vector_padded.reshape((n, n))\n",
    "    \n",
    "    return square_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def label_point(x, y, ids, ax):\n",
    "    \"\"\"Annotate points on plot with their IDs.\"\"\"\n",
    "    for i, txt in enumerate(ids):\n",
    "        ax.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "def interactive_plot(x, y, ids=None, colors=None, action='click', img_list=None, emb_list=None, true_img_list=None ,zoom=False, filtered_ids=[]):\n",
    "    \"\"\"Identify the ID of a point by clicking on it.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = [str(i) for i in range(len(x))]\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 8)) if zoom else plt.subplots(1, 4, figsize=(16,4))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    ax[1].axis('off')\n",
    "    ax[2].axis('off')\n",
    "    ax[3].axis('off')\n",
    "    \n",
    "    ax[0].scatter(x, y, s=1, c=colors) if colors is not None else ax[0].scatter(x, y, s=1 if zoom else 0.1)\n",
    "    # Set limit to the plot\n",
    "    if zoom:\n",
    "        ax[4].scatter(x, y, s=1, c=colors) if colors is not None else ax[4].scatter(x, y, s=1)\n",
    "        ax[4].set_xlim([-100, 100])\n",
    "        ax[4].set_ylim([-100, 100])\n",
    "    # Set a threshold based on max values of x and y\n",
    "    threshold = max(max(x) - min(x), max(y) - min(y)) / 80\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    #label_point(x, y, ids, ax[0])\n",
    "    tree = KDTree(np.column_stack((x, y)))\n",
    "    \n",
    "    def onclick(event):\n",
    "        \"\"\"Event handler for mouse click.\"\"\"\n",
    "        if event.inaxes == ax[0]:\n",
    "            dist, i = tree.query([event.xdata, event.ydata])\n",
    "            if dist < threshold:\n",
    "                ax[1].imshow(img_list[int(ids[i])], cmap='gray')\n",
    "                id_img_title = ids[i] if len(filtered_ids) == 0 else filtered_ids[i]\n",
    "                ax[1].set_title(f'Image {id_img_title} (color: {colors[i]})') if colors is not None else ax[1].set_title(f'Image {id_img_title}')\n",
    "                ax[2].imshow(reshape_to_square(emb_list[int(ids[i])]), cmap='gray')\n",
    "                ax[2].set_title(f'Embedding {id_img_title}')\n",
    "                ax[3].imshow(true_img_list[int(ids[i])], cmap='gray')\n",
    "                ax[3].set_title(f'True Image {id_img_title}')\n",
    "                \n",
    "                ax[1].axis('off')\n",
    "                ax[2].axis('off')\n",
    "                ax[3].axis('off')\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    if action == 'click':\n",
    "        fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "    elif action == 'hover':\n",
    "        fig.canvas.mpl_connect('motion_notify_event', onclick)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_2_clusters(embeddings_tsne, cluster_labels, cluster_labels_2d):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=cluster_labels, s=1)\n",
    "    ax[0].set_title('Clustering Results (t-SNE embedding)')\n",
    "\n",
    "    ax[1].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=cluster_labels_2d, s=1)\n",
    "    ax[1].set_title('Clustering Results (t-SNE embedding 2D)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) VAE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/data/falcetta/brain_data\"\n",
    "save_embeddings_path = os.path.join(path_data, f\"embeddings_VDISNET\") \n",
    "\n",
    "dataset_name = 'CAS'\n",
    "\n",
    "save_embeddings_path_SOTA = os.path.join(save_embeddings_path,'SOTA')\n",
    "print(f\"Save embeddings in {save_embeddings_path_SOTA}\")\n",
    "mk_dir(save_embeddings_path_SOTA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss, title=''):\n",
    "    plt.figure()\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.title(f\"{title} Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class ConvVAEDataset(Dataset):\n",
    "    def __init__(self, images_array, transform=None, metadata=None, device='cuda'):\n",
    "        self.images_array = images_array\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "        \n",
    "        if metadata is None:\n",
    "            self.compute_metadata()\n",
    "        else:\n",
    "            self.mean, self.std = metadata \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        idx_0 = int(index)\n",
    "        img0 = self.images_array[idx_0]\n",
    "        img0 = Image.fromarray(img0)\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "        img0 = (img0 - self.mean) / self.std    \n",
    "        img0 = (img0 - img0.min()) / (img0.max() - img0.min())\n",
    "        return img0\n",
    "    \n",
    "    def compute_metadata(self):\n",
    "        with torch.no_grad():\n",
    "            flattened_data = self.images_array.reshape(self.images_array.shape[0], -1)\n",
    "            means = np.mean(flattened_data, axis=0)\n",
    "            stds = np.std(flattened_data, axis=0)\n",
    "            \n",
    "            mean = np.mean(means)\n",
    "            std = np.mean(stds)\n",
    "            \n",
    "            self.mean = torch.tensor(mean)\n",
    "            self.std = torch.tensor(std)\n",
    "        \n",
    "        print(f\"Mean: {self.mean} - Std: {self.std}\")\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return self.mean, self.std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CAS = np.load(os.path.join(save_embeddings_path,f'X_test_CAS_all.npy'))\n",
    "X_CAS_mask = np.load(os.path.join(save_embeddings_path,f'X_test_mask_CAS_all.npy'))\n",
    "\n",
    "X_CAS_empty = np.load(os.path.join(save_embeddings_path,f'X_test_empty_CAS_all.npy'))\n",
    "X_CAS_mask_empty = np.load(os.path.join(save_embeddings_path,f'X_test_empty_mask_CAS_all.npy'))\n",
    "\n",
    "print(f\"Data loaded\")\n",
    "\n",
    "print(f\"X_test shape: {X_CAS.shape}\")\n",
    "print(f\"X_test_mask shape: {X_CAS_mask.shape}\")\n",
    "\n",
    "print(f\"X_test_empty shape: {X_CAS_empty.shape}\")\n",
    "print(f\"X_test_empty_mask shape: {X_CAS_mask_empty.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CAS_tot = np.concatenate((X_CAS, X_CAS_empty), axis=0)\n",
    "X_CAS_mask_tot = np.concatenate((X_CAS_mask, X_CAS_mask_empty), axis=0)\n",
    "\n",
    "print(f\"X_test_tot shape: {X_CAS_tot.shape}\")\n",
    "print(f\"X_test_mask_tot shape: {X_CAS_mask_tot.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CAS = X_CAS_tot\n",
    "X_CAS_mask = X_CAS_mask_tot\n",
    "\n",
    "print(f\"X_test shape: {X_CAS.shape}\")\n",
    "print(f\"X_test_mask shape: {X_CAS_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(save_embeddings_path_SOTA,f'X_train_CAS_SOTA.npy'))\n",
    "X_train_mask = np.load(os.path.join(save_embeddings_path_SOTA,f'X_train_mask_CAS_SOTA.npy'))\n",
    "\n",
    "X_val = np.load(os.path.join(save_embeddings_path_SOTA,f'X_val_CAS_SOTA.npy'))\n",
    "X_val_mask = np.load(os.path.join(save_embeddings_path_SOTA,f'X_val_mask_CAS_SOTA.npy'))\n",
    "\n",
    "X_train.shape, X_train_mask.shape, X_val.shape, X_val_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    RA_dataset = ConvVAEDataset(images_array=X_train,\n",
    "                                            transform=transforms.ToTensor(),)\n",
    "\n",
    "    metadata = RA_dataset.get_metadata()\n",
    "    np.save(os.path.join(save_embeddings_path,f'metadata_CAS.npy'), metadata)\n",
    "\n",
    "    RA_val_dataset = ConvVAEDataset(images_array=X_val,\n",
    "                                                transform=transforms.ToTensor(),\n",
    "                                                metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = np.load(os.path.join(save_embeddings_path,f'metadata_CAS.npy'))\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "if not just_sampling:\n",
    "    vis_dataloader = DataLoader(RA_dataset,\n",
    "                            shuffle=True,\n",
    "                            num_workers=0,\n",
    "                            batch_size=1)\n",
    "\n",
    "    plot_same = True\n",
    "    plot_diff = True\n",
    "\n",
    "    print(f\"Plotting examples\")\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.figure()\n",
    "    for en,example_batch in enumerate(vis_dataloader):\n",
    "        plt.figure()\n",
    "        plt.imshow(example_batch[0][0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        if en > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONV-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1), # [bs, 32, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # [bs, 64, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # [bs, 128, 4, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # [bs, 256, 2, 2]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        dim_last_conv = 256 * 2 * 2\n",
    "        self.fc_mu = nn.Linear(dim_last_conv, latent_dim) # [bs, latent_dim]\n",
    "        self.fc_logvar = nn.Linear(dim_last_conv, latent_dim) # [bs, latent_dim]\n",
    "        self.fc_decode = nn.Linear(latent_dim, dim_last_conv) # [bs, 256 * 2 * 2]\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # [bs, 128, 4, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # [bs, 64, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # [bs, 32, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1), # [bs, 1, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x, return_latent=False):\n",
    "        #print(f\"Input shape: {x.shape}\") # [bs, 1, 32, 32]\n",
    "        h = self.encoder(x)\n",
    "        #print(f\"Encoded shape: {h.shape}\") # [bs, 256, 2, 2]\n",
    "        h = h.view(h.size(0), -1)\n",
    "        #print(f\"Flattened shape: {h.shape}\") # [bs, 256 * 2 * 2]\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z_lat = self.reparameterize(mu, logvar)\n",
    "        z = self.fc_decode(z_lat)\n",
    "        #print(f\"Decoded shape: {z.shape}\") # [bs, 256 * 2 * 2]\n",
    "        \n",
    "        z = z.view(z.size(0), 256, 2, 2)\n",
    "        #z = z.view(z.size(0), 128, 4, 4)\n",
    "        #z = z.view(z.size(0), 64, 8, 8)\n",
    "        \n",
    "        #print(f\"Reshaped shape: {z.shape}\") # [bs, 256, 2, 2]\n",
    "        #print(f\"Output shape: {self.decoder(z).shape}\") # [bs, 1, 32, 32]\n",
    "        if return_latent:\n",
    "            return z_lat\n",
    "        \n",
    "        return self.decoder(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE loss function.\n",
    "    Combines the reconstruction loss (Binary Cross-Entropy) and the Kullback-Leibler Divergence (KLD) loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAELoss, self).__init__()\n",
    "        self.loss_accumulator = 0.0\n",
    "        self.num_samples = 0\n",
    "\n",
    "    def forward(self, recon_x, x, mu, logvar, message='TR'):\n",
    "\n",
    "        x = x.to(recon_x.device)\n",
    "        mu = mu.to(recon_x.device)\n",
    "        logvar = logvar.to(recon_x.device)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='mean')\n",
    "        \n",
    "        # Kullback-Leibler divergence\n",
    "        KLD = torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #print(f\"BCE: {BCE}, KLD: {KLD}\")\n",
    "        # Total loss\n",
    "        beta = 0 # weight for KLD loss (if alpha=0, the model is a simple autoencoder)\n",
    "        total_loss = BCE - beta * KLD\n",
    "        if random.random() < 0.001:\n",
    "            print(f\"{message} SINGLE LOSSES ==> BCE: {BCE}, W-KLD: {-beta * KLD}, KLD: {-KLD}\") \n",
    "        \n",
    "        # Accumulate loss\n",
    "        self.loss_accumulator += total_loss.item()\n",
    "        self.num_samples += 1\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def get_accumulated_loss(self):\n",
    "        return self.loss_accumulator\n",
    "\n",
    "    def get_mean_loss(self):\n",
    "        mean_loss = self.loss_accumulator / self.num_samples\n",
    "        self.reset()\n",
    "        return mean_loss\n",
    "        \n",
    "    def reset(self):\n",
    "        self.loss_accumulator = 0.0\n",
    "        self.num_samples = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    train_dataloader = DataLoader(RA_dataset,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8,\n",
    "                            batch_size=128)\n",
    "\n",
    "    val_dataloader = DataLoader(RA_val_dataset,\n",
    "                            shuffle=False,\n",
    "                            num_workers=8,\n",
    "                            batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "start_epoch=0\n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    net = ConvVAE(latent_dim=latent_dim).cuda()\n",
    "\n",
    "    if load_model:\n",
    "        checkpoint_path = os.path.join(save_embeddings_path_SOTA, 'best_model_RA.pt')\n",
    "        print(f\"Loading model from {checkpoint_path}\")\n",
    "        # Load state dictionary into model\n",
    "        net.load_state_dict(torch.load(checkpoint_path))\n",
    "        start_epoch= 36\n",
    "\n",
    "\n",
    "    criterion = VAELoss()\n",
    "\n",
    "    best_loss = 999999999\n",
    "    best_loss_epoch = 0\n",
    "    cumul_epochs = 0\n",
    "\n",
    "    mean_loss_contrastive_list = []\n",
    "    best_loss_contrastive_list = []\n",
    "    validation_loss_list = []\n",
    "    continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if not just_sampling:\n",
    "    n_epochs = 500\n",
    "    early_stopping_tolerance = 50\n",
    "    optimizer = optim.Adam(net.parameters(), betas=(0.9, 0.999), lr=1e-4)\n",
    "\n",
    "    if continue_training:\n",
    "        mean_loss_contrastive_list = mean_loss_contrastive_list.tolist()\n",
    "        best_loss_contrastive_list = best_loss_contrastive_list.tolist()\n",
    "        validation_loss_list = validation_loss_list.tolist()\n",
    "\n",
    "    print(f\"Starting round of training from epoch {cumul_epochs} (Best loss: {best_loss:.2f})\")\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, n_epochs), desc='Epochs'): \n",
    "        # Training loop\n",
    "        net.train()\n",
    "        \n",
    "        for data in tqdm(train_dataloader, desc='Batches', leave=False):\n",
    "            img0 = data.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = net(img0)\n",
    "            loss_contrastive = criterion(recon_batch, img0, mu, logvar)\n",
    "            #show an example of the reconstruction (Just 4 images)\n",
    "            loss_contrastive.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0 and cumul_epochs % 10 == 0:\n",
    "                print(f\"Recon shape: {recon_batch.shape}, Mu shape: {mu.shape}, Logvar shape: {logvar.shape}\")\n",
    "                imshow(torchvision.utils.make_grid(recon_batch.cpu().detach()[0:4]), 'TR - Reconstruction')\n",
    "                imshow(torchvision.utils.make_grid(img0.cpu().detach()[0:4]), 'Original')\n",
    "            \n",
    "        # Calculate mean loss for contrastive loss during training\n",
    "        mean_loss_contrastive = criterion.get_mean_loss()\n",
    "        mean_loss_contrastive_list.append(mean_loss_contrastive)\n",
    "        \n",
    "        # Validation loop\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(val_dataloader, desc='Validation', leave=False):\n",
    "                val_img0 = val_data.cuda()\n",
    "                val_recon_batch, val_mu, val_logvar = net(val_img0)\n",
    "                val_loss += criterion(val_recon_batch, val_img0, val_mu, val_logvar,\"val\").item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        validation_loss_list.append(val_loss)\n",
    "        #print(f\"Validation Loss: {val_loss:.2f}\")\n",
    "        if epoch % 10 == 0 and cumul_epochs % 10 == 0:\n",
    "            print(f\"Recon shape: {val_recon_batch.shape}, Mu shape: {val_mu.shape}, Logvar shape: {val_logvar.shape}\")\n",
    "            imshow(torchvision.utils.make_grid(val_recon_batch.cpu().detach()[0:4]), 'VAL - Reconstruction')\n",
    "            imshow(torchvision.utils.make_grid(val_img0.cpu().detach()[0:4]), 'Original')\n",
    "        # Check if current loss is the best so far\n",
    "        if val_loss < best_loss:\n",
    "            print(f\"Epoch number {cumul_epochs} --- NEW Best loss {val_loss}\")\n",
    "            best_loss = val_loss\n",
    "            best_loss_epoch = cumul_epochs\n",
    "            torch.save(net.state_dict(), os.path.join(save_embeddings_path_SOTA, 'best_model_RA.pt'))\n",
    "        elif cumul_epochs - best_loss_epoch > 10:\n",
    "            print(f\"No improvement in the last 10 epochs. Validation loss: {val_loss:.2f}\")\n",
    "        else:\n",
    "            if cumul_epochs - best_loss_epoch > early_stopping_tolerance:\n",
    "                print(f\"Early stopping at epoch {cumul_epochs}\")\n",
    "                best_loss_contrastive_list.append(best_loss)\n",
    "                cumul_epochs +=1\n",
    "                break    \n",
    "        \n",
    "        best_loss_contrastive_list.append(best_loss)\n",
    "        cumul_epochs +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'mean_loss_contrastive_list_CAS_RA.npy'), mean_loss_contrastive_list)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'best_loss_contrastive_list_CAS_RA.npy'), best_loss_contrastive_list)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'validation_loss_list_CAS_RA.npy'), validation_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    mean_loss_contrastive_list = np.load(os.path.join(save_embeddings_path_SOTA, f'mean_loss_contrastive_list_CAS_RA.npy'))\n",
    "    best_loss_contrastive_list = np.load(os.path.join(save_embeddings_path_SOTA, f'best_loss_contrastive_list_CAS_RA.npy'))\n",
    "    validation_loss_list = np.load(os.path.join(save_embeddings_path_SOTA, f'validation_loss_list_CAS_RA.npy'))\n",
    "\n",
    "    continue_training=True\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.ioff()\n",
    "    plt.close('all')\n",
    "    show_plot(range(0,len(mean_loss_contrastive_list)),mean_loss_contrastive_list, title='Training Loss')\n",
    "    show_plot(range(0,len(best_loss_contrastive_list)),validation_loss_list, title='Val Loss')\n",
    "    show_plot(range(0,len(best_loss_contrastive_list)),best_loss_contrastive_list, title='Best Val Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL and Test (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# LOAD BEST MODEL\n",
    "if not just_sampling:\n",
    "    net = ConvVAE(latent_dim=latent_dim).cuda()\n",
    "    checkpoint_path = os.path.join(save_embeddings_path_SOTA, 'best_model_RA.pt')\n",
    "\n",
    "    # Load state dictionary into model\n",
    "    net.load_state_dict(torch.load(checkpoint_path))\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "from tqdm import tqdm\n",
    "\n",
    "if not just_sampling:\n",
    "    siamese_testset = ConvVAEDataset(images_array=X_CAS,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            metadata=metadata,)\n",
    "\n",
    "    test_dataloader = DataLoader(siamese_testset,\n",
    "                            shuffle=False,\n",
    "                            batch_size=1)\n",
    "\n",
    "\n",
    "    # Get the total number of images\n",
    "    num_images = len(siamese_testset)\n",
    "\n",
    "    # Assuming output1 has a fixed size, get the latent vector size\n",
    "    # Here, we get a dummy batch to determine the size. Replace this with actual size if known\n",
    "    dummy_img = next(iter(test_dataloader)).cuda()\n",
    "    latent_vector_size = net(dummy_img, return_latent=True).cpu().detach().numpy().shape\n",
    "    print(f\"Latent vector size: {latent_vector_size}\")\n",
    "\n",
    "    # Preallocate the numpy array for embeddings\n",
    "    img_embeddings = np.zeros((num_images, latent_dim))\n",
    "\n",
    "    # Process each image and store the embeddings\n",
    "    for i, img in enumerate(tqdm(test_dataloader)):\n",
    "        img0 = img.cuda()\n",
    "        output1 = net(img0, return_latent=True).cpu().detach().numpy()\n",
    "        img_embeddings[i] = output1.flatten()\n",
    "    \n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'img_embeddings_CAS_RA.npy'), img_embeddings)\n",
    "    print(f\"Image Embeddings saved (shape {img_embeddings.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load img embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = np.load(os.path.join(save_embeddings_path_SOTA, f'img_embeddings_CAS_RA.npy'))\n",
    "\n",
    "print(f\"img_embeddings loaded from {save_embeddings_path_SOTA}\")\n",
    "print(f'img_embeddings shape: {img_embeddings.shape}') #128*4*4 = 2048\n",
    "\n",
    "img_embeddings = img_embeddings.astype(np.float32)\n",
    "print(f'img_embeddings dtype: {img_embeddings.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLot IMG Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "if not just_sampling:\n",
    "    # Perform t-SNE clustering\n",
    "    #Peplexity 500\n",
    "    #early_exaggeration=40\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    img_embeddings_tsne = tsne.fit_transform(img_embeddings)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], s=1)\n",
    "    plt.title('Image Clustering Results (t-SNE img_embedding)')\n",
    "\n",
    "    # save the embeddings\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_RA.npy'), img_embeddings_tsne)\n",
    "    print(f\"TSNE img_embeddings saved in {os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_RA.npy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings_tsne = np.load(os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_RA.npy'))\n",
    "print(f\"TSNE img_embeddings loaded\")\n",
    "print(f\"TSNE img_embeddings shape: {img_embeddings_tsne.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=None, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_patches_overlap(X_CAS_mask, X_CAS, indexes=[5597,24,25,26,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_img = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS T-SNE (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "if not just_sampling:\n",
    "    # Create a KMeans object with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters_img, random_state=42, n_init='auto')\n",
    "\n",
    "    # Fit the KMeans model to the img_embeddings\n",
    "    kmeans.fit(img_embeddings_tsne)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    img_cluster_labels_2d = kmeans.labels_\n",
    "    print(f\"Classes: {set(img_cluster_labels_2d)}\")\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_2d_CAS_RA.npy'), img_cluster_labels_2d)\n",
    "\n",
    "\n",
    "    # Plot histogram of cluster labels\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(img_cluster_labels_2d)\n",
    "    plt.xlabel('Cluster Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Cluster Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cluster_labels_2d = np.load(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_2d_CAS_RA.npy'))\n",
    "print(f\"Cluster labels loaded\")\n",
    "print(f\"Cluster labels shape: {img_cluster_labels_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=img_cluster_labels_2d, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS with embedding code (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "if not just_sampling:\n",
    "    # Create a KMeans object with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters_img, random_state=42, n_init='auto')\n",
    "\n",
    "    # Fit the KMeans model to the img_embeddings\n",
    "    kmeans.fit(img_embeddings)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    img_cluster_labels = kmeans.labels_\n",
    "    print(f\"Classes: {set(img_cluster_labels)}\")\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_CAS_RA.npy'), img_cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cluster_labels = np.load(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_CAS_RA.npy'))\n",
    "print(f\"Cluster labels loaded\")\n",
    "print(f\"Cluster labels shape: {img_cluster_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_clusters(img_embeddings_tsne, img_cluster_labels, img_cluster_labels_2d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling ==> AGGLOMERATIVE CLUSTERING (TOO MUCH COMPUTATION NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# def parallel_linkage(data, method='ward', n_jobs=-1):\n",
    "#     # Split data into chunks for parallel processing\n",
    "#     chunks = np.array_split(data, n_jobs)\n",
    "#     results = Parallel(n_jobs=n_jobs)(delayed(linkage)(chunk, method=method) for chunk in chunks)\n",
    "    \n",
    "#     # Combine results\n",
    "#     combined = np.vstack(results)\n",
    "#     return combined\n",
    "\n",
    "# def determine_num_clusters(data, max_clusters=30, n_jobs=12):\n",
    "#     print(f\"DETERMINE NUM CLUSTERS IN PARALLEL\")\n",
    "#     data_flat = data.reshape(data.shape[0], -1)\n",
    "    \n",
    "#     # Perform hierarchical clustering using 'ward' method in parallel\n",
    "#     linked = parallel_linkage(data_flat, method='ward', n_jobs=n_jobs)\n",
    "    \n",
    "#     # Create the dendrogram with no plotting and truncation to max_clusters\n",
    "#     dendro = dendrogram(linked, truncate_mode='lastp', p=max_clusters, no_plot=True)\n",
    "    \n",
    "#     # Get the number of clusters from the dendrogram\n",
    "#     num_clusters = len(dendro['leaves'])\n",
    "    \n",
    "#     return num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastcluster import linkage as fast_linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def determine_num_clusters(data, max_clusters=30, n_jobs=12):\n",
    "    # Efficiently flatten the data\n",
    "    print(f\"DETERMINE NUM CLUSTERS WITH FASTCLUSTER\")\n",
    "    data_flat = data.reshape(data.shape[0], -1)\n",
    "    \n",
    "    # Perform hierarchical clustering using 'ward' method with fastcluster\n",
    "    linked = fast_linkage(data_flat, method='ward', preserve_input=True)\n",
    "    \n",
    "    # Create the dendrogram with no plotting and truncation to max_clusters\n",
    "    dendro = dendrogram(linked, truncate_mode='lastp', p=max_clusters, no_plot=True)\n",
    "    \n",
    "    # Get the number of clusters from the dendrogram\n",
    "    num_clusters = len(dendro['leaves'])\n",
    "    \n",
    "    return num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def select_representatives(cluster_points, cluster_ids, num_representatives):\n",
    "    cluster_flat = cluster_points.reshape(cluster_points.shape[0], -1)\n",
    "    representatives = []\n",
    "    representatives_idx = []\n",
    "    \n",
    "    # Initialize by selecting a random point as the first representative\n",
    "    initial_idx = np.random.choice(len(cluster_flat))\n",
    "    representatives.append(cluster_flat[initial_idx])\n",
    "    representatives_idx.append(cluster_ids[initial_idx])\n",
    "    remaining_indices = set(range(len(cluster_flat))) - {initial_idx}\n",
    "    \n",
    "    while len(representatives) < num_representatives:\n",
    "        max_min_dist = -1\n",
    "        next_representative = None\n",
    "        \n",
    "        for idx in remaining_indices:\n",
    "            min_dist = min(np.linalg.norm(cluster_flat[idx] - rep) for rep in representatives)\n",
    "            if min_dist > max_min_dist:\n",
    "                max_min_dist = min_dist\n",
    "                next_representative = idx\n",
    "                \n",
    "        representatives.append(cluster_flat[next_representative])\n",
    "        representatives_idx.append(cluster_ids[next_representative])\n",
    "        remaining_indices.remove(next_representative)\n",
    "    \n",
    "    return np.array(representatives).reshape(-1, *cluster_points.shape[1:]), representatives_idx\n",
    "\n",
    "\n",
    "def compute_pairwise_distances(data):\n",
    "    return pairwise_distances(data)\n",
    "\n",
    "# def compute_distance_matrix(data, metric='euclidean'):\n",
    "#     # Compute the full distance matrix using pairwise_distances\n",
    "#     return pairwise_distances(data, metric=metric)\n",
    "\n",
    "def compute_distance_matrix(data, metric='euclidean'):\n",
    "    return pairwise_distances(data, metric=metric, n_jobs=-1)\n",
    "\n",
    "def compute_distance_matrix(data, metric='euclidean', chunk_size=100):\n",
    "    n_samples = data.shape[0]\n",
    "    distance_matrix = np.zeros((n_samples, n_samples), dtype=np.float32)\n",
    "    \n",
    "    for i in tqdm(range(0, n_samples, chunk_size)):\n",
    "        for j in range(0, n_samples, chunk_size):\n",
    "            i_end = min(i + chunk_size, n_samples)\n",
    "            j_end = min(j + chunk_size, n_samples)\n",
    "            distance_matrix[i:i_end, j:j_end] = pairwise_distances(data[i:i_end], data[j:j_end], metric=metric, n_jobs=-1)\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def ClsMC_RS(data, percentage=0.1, num_clusters=0, chunk_size=100):\n",
    "    if num_clusters == 0:\n",
    "        print(\"STEP 0: Determining num clusters\")\n",
    "        # Step 1: Determine the number of clusters using agglomerative clustering and dendrogram\n",
    "        num_clusters = determine_num_clusters(data)\n",
    "    print(f\"Number of clusters: {num_clusters}\")\n",
    "    \n",
    "    # Step 2: Perform clustering with the determined number of clusters\n",
    "    print(\"STEP 1: Clustering data\")\n",
    "    data_flat = data.reshape(data.shape[0], -1)\n",
    "    print(f\"Data flat shape: {data_flat.shape}, dtype: {data_flat.dtype}\")\n",
    "\n",
    "    # Compute distance matrix\n",
    "    print(\"Computing distance matrix\")\n",
    "    if not os.path.exists(os.path.join(save_embeddings_path_SOTA,f'distance_matrix_CAS_RA.npy')):\n",
    "        distance_matrix = compute_distance_matrix(data_flat, chunk_size=chunk_size)\n",
    "        np.save(os.path.join(save_embeddings_path_SOTA,f'distance_matrix_CAS_RA.npy'), distance_matrix)\n",
    "    else:\n",
    "        print(\"Loading distance matrix\")\n",
    "        distance_matrix = np.load(os.path.join(save_embeddings_path_SOTA,f'distance_matrix_CAS_RA.npy'))\n",
    "        \n",
    "    print(\"Performing Agglomerative clustering\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_embeddings_path_SOTA,f'cluster_labels_CAS_RA.npy')):\n",
    "        clustering = AgglomerativeClustering(n_clusters=num_clusters, affinity='precomputed', linkage='complete')\n",
    "        cluster_labels = clustering.fit_predict(distance_matrix)\n",
    "        np.save(os.path.join(save_embeddings_path_SOTA,f'cluster_labels_CAS_RA.npy'), cluster_labels)\n",
    "        print(f\"Different clusters: {set(cluster_labels)}\")\n",
    "    else:\n",
    "        print(\"Loading cluster labels\")\n",
    "        cluster_labels = np.load(os.path.join(save_embeddings_path_SOTA,f'cluster_labels_CAS_RA.npy'))\n",
    "        \n",
    "    print(f\"Different clusters: {set(cluster_labels)}\")\n",
    "    \n",
    "    # Step 3: Representative selection max coverage sampling\n",
    "    print(\"STEP 2: Selecting representatives\")\n",
    "    representatives = []\n",
    "    representatives_idx = []\n",
    "    for cluster_idx in range(num_clusters):\n",
    "        cluster_points = data[cluster_labels == cluster_idx]\n",
    "        cluster_ids = np.where(cluster_labels == cluster_idx)[0]\n",
    "        print(f\"Cluster {cluster_idx} - Num points: {len(cluster_points)}\")\n",
    "        print(f\"Cluster IDs: {cluster_ids}\")\n",
    "        reps, id_reps = select_representatives(cluster_points, cluster_ids, int(len(cluster_points) * percentage))\n",
    "        representatives.extend(reps)\n",
    "        representatives_idx.extend(id_reps)\n",
    "        \n",
    "        print(f\"Adding {len(reps)} representatives (TOTAL: {len(representatives)})\")\n",
    "        print(f\"Chosen representatives: {id_reps}\")\n",
    "    \n",
    "    return np.array(representatives), representatives_idx\n",
    "# Example usage:\n",
    "# data = np.random.rand(100, 2)  # Replace with actual data\n",
    "# representatives, representatives_idx = ClsMC_RS(data)\n",
    "\n",
    "# Example usage\n",
    "#num_points = 1000\n",
    "#latent_space_shape = (1, 128, 4, 4)\n",
    "#data = np.random.rand(num_points, *latent_space_shape)\n",
    "#percentage = 0.1\n",
    "#print(f\"Data shape: {data.shape} (percentage: {percentage})\")\n",
    "\n",
    "#representatives, representatives_idx = ClsMC_RS(data, percentage)\n",
    "\n",
    "\n",
    "#print(f\"Selected representatives shape: {representatives.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings_selected = ClsMC_RS(img_embeddings, percentage=0.1, num_clusters=30, chunk_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_pairwise_distances2(data):\n",
    "    return pairwise_distances(data)\n",
    "\n",
    "#def monte_carlo_sampling(cluster_data, num_samples):\n",
    "#    samples = []\n",
    "#    for _ in range(num_samples):\n",
    "#        sample_indices = np.random.choice(len(cluster_data), size=len(cluster_data) // 2, replace=False)\n",
    "#        samples.append(cluster_data[sample_indices])\n",
    "#    return samples\n",
    "\n",
    "\n",
    "\n",
    "# def ClsMC_RS(data, percentage=0.1, num_clusters=0):\n",
    "#     if num_clusters == 0:\n",
    "#         print(f\"STEP 0: Determining num clusters\")\n",
    "#         # Step 1: Determine the number of clusters using agglomerative clustering and dendrogram\n",
    "#         num_clusters = determine_num_clusters(data)\n",
    "#     print(f\"Number of clusters: {num_clusters}\")\n",
    "    \n",
    "#     # Step 2: Perform clustering with the determined number of clusters\n",
    "#     print(f\"STEP 1: Clustering data\")\n",
    "#     data_flat = data.reshape(data.shape[0], -1)\n",
    "#     clustering = AgglomerativeClustering(n_clusters=num_clusters, affinity='precomputed', linkage='complete')\n",
    "#     cluster_labels = clustering.fit_predict(data_flat)\n",
    "#     print(f\"Different clusters: {set(cluster_labels)}\")\n",
    "    \n",
    "#     # Step 3: Representative selection max coverage sampling\n",
    "#     print(f\"STEP 2: Selecting representatives\")\n",
    "#     representatives = []\n",
    "#     representatives_idx = []\n",
    "#     for cluster_idx in range(num_clusters):\n",
    "#         cluster_points = data[cluster_labels == cluster_idx]\n",
    "#         cluster_ids = np.where(cluster_labels == cluster_idx)[0]\n",
    "#         print(f\"Cluster {cluster_idx} - Num points: {len(cluster_points)}\")\n",
    "#         print(f\"Cluster {len(cluster_ids)}: {cluster_ids}\")\n",
    "#         reps, id_reps = select_representatives(cluster_points, cluster_ids, len(cluster_points)*percentage)\n",
    "#         representatives.extend(reps)\n",
    "#         representatives_idx.extend(id_reps)\n",
    "        \n",
    "#         print(f\"Adding {len(reps)} representatives (TOTAL: {len(representatives)})\")\n",
    "#         print(f\"Chosen representatives: {id_reps}\")\n",
    "    \n",
    "#     return np.array(representatives), representatives_idx\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# num_points = 1000\n",
    "# latent_space_shape = (1, 128, 4, 4)\n",
    "# data = np.random.rand(num_points, *latent_space_shape).astype(np.float32)\n",
    "# percentage = 0.1\n",
    "# print(f\"Data shape: {data.shape} (percentage: {percentage})\")\n",
    "\n",
    "# representatives, representatives_idx = ClsMC_RS(data, percentage)\n",
    "\n",
    "\n",
    "# print(f\"Selected representatives shape: {representatives.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Img list shape: {X_CAS.shape}\")\n",
    "print(f\"Mask list shape: {X_CAS_mask.shape}\\n\")\n",
    "\n",
    "print(f\"Img Embeddings shape: {img_embeddings.shape}\")\n",
    "print(f\"Img Embeddings TSNE shape {img_embeddings_tsne.shape}\")\n",
    "\n",
    "print(f\"Class shape: {img_cluster_labels.shape}\")\n",
    "print(f\"Class 2D shape: {img_cluster_labels_2d.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse_ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
