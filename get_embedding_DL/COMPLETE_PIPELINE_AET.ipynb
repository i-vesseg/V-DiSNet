{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from utils import set_random_seed, mk_dir\n",
    "from importlib import import_module\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion() \n",
    "\n",
    "def save_img(tensor, name, norm, n_rows=16, scale_each=False):\n",
    "    save_image(tensor, name, nrow=n_rows, padding=5, normalize=norm, pad_value=1, scale_each=scale_each)\n",
    "    \n",
    "def plot_10_patches(img_np, true_vessel_np=False, indexes=None):\n",
    "    \n",
    "    random_indexes = np.random.randint(0, img_np.shape[0], size=5) if indexes is None else indexes\n",
    "\n",
    "    n = len(random_indexes)\n",
    "    \n",
    "    fig, ax = plt.subplots(n, 2, figsize=(6, 3*n))\n",
    "    \n",
    "    for i, index in enumerate(random_indexes):\n",
    "        \n",
    "        if index == img_np.shape[0]:\n",
    "            index -= 1\n",
    "        if index == img_np.shape[0]-1:\n",
    "            index -= 2\n",
    "            \n",
    "        #print(f\"Shape: {img_np[index,:,:].shape}, Max: {img_np[index,:,:].max()}, Min: {img_np[index,:,:].min()}\")\n",
    "        ax[i, 0].set_title(f'True Vessel Slice {index}')\n",
    "        ax[i, 0].imshow(img_np[index,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        ax[i,0].axis('off')\n",
    "        \n",
    "        if true_vessel_np is not False:\n",
    "            print(f\"Shape: {true_vessel_np[index,:,:].shape}, Max: {true_vessel_np[index,:,:].max()}, Min: {true_vessel_np[index,:,:].min()}\")\n",
    "            ax[i, 1].imshow(true_vessel_np[index,:,:], cmap='gray')\n",
    "            ax[i, 1].set_title(f'Image Slice {index}')\n",
    "        else:\n",
    "            ax[i, 1].set_title(f'Image Slice {index+1}')\n",
    "            ax[i, 1].imshow(img_np[index+1,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        ax[i,1].axis('off')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_n_patches_overlap(img_np, true_vessel_np=False, indexes=None, selected_class=None, add_title='', m=5, alpha=0.5, save_dir='clusters_imgs'):\n",
    "    \n",
    "    save_dir = os.path.join(save_embeddings_path,save_dir)\n",
    "    mk_dir(save_dir)\n",
    "    plt.ioff()\n",
    "    n = len(indexes) if indexes is not None else 5\n",
    "    # m is the number of images to plot in each row (2m is the number of columns)\n",
    "    n_rows = int(np.ceil(n/m))\n",
    "    n_cols = m\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, index in enumerate(indexes):\n",
    "        ax[i].set_title(f'Overlay for Slice {index}')\n",
    "        masked_image = np.ma.masked_where(img_np[index,:,:] == 0, true_vessel_np[index,:,:])\n",
    "        # Overlay the red image on top of true_vessel_np[index,:,:]\n",
    "        ax[i].imshow(true_vessel_np[index,:,:], cmap='gray', interpolation='none')\n",
    "        ax[i].imshow(masked_image, cmap='Reds', alpha=alpha)\n",
    "        \n",
    "        ax[i].axis('off')\n",
    "    \n",
    "    try:\n",
    "        plt.tight_layout()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if add_title!='':\n",
    "        print(f\"Save Fig to {save_dir}\")\n",
    "        plt.savefig(os.path.join(save_dir,f'{add_title}_class_{selected_class}_patches.png'))\n",
    "        plt.close(fig)\n",
    "        \n",
    "    else:\n",
    "        print(\"Plotting..\")\n",
    "        plt.show()\n",
    "    plt.ion()\n",
    "\n",
    "    \n",
    "def plot_n_patches(img_np, true_vessel_np=False, indexes=None, selected_class=None, add_title='', m=5):\n",
    "    plt.ioff()\n",
    "    n = len(indexes) if indexes is not None else 5\n",
    "    if n > 1000:\n",
    "        print(f'WARNING: YOU ARE TRYING TO PLOT {n} images')\n",
    "    # m is the number of images to plot in each row (2m is the number of columns)\n",
    "    n_rows = int(np.ceil(n/m))\n",
    "    n_cols = 2*m\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, index in enumerate(indexes):\n",
    "        \n",
    "        if index == img_np.shape[0]:\n",
    "            index -= 1\n",
    "        if index == img_np.shape[0]-1:\n",
    "            index -= 2\n",
    "            \n",
    "        ax[2*i].set_title(f'Image Slice {index}')\n",
    "        ax[2*i].imshow(img_np[index,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        ax[2*i].axis('off')\n",
    "        print(f\"Shape: {img_np[index,:,:].shape}, Max: {img_np[index,:,:].max()}, Min: {img_np[index,:,:].min()}\")\n",
    "        if true_vessel_np is not False:\n",
    "            ax[2*i + 1].imshow(true_vessel_np[index,:,:], cmap='gray')\n",
    "            ax[2*i + 1].set_title(f'True Vessel Slice {index}')\n",
    "            \n",
    "        else:\n",
    "            ax[2*i + 1].set_title(f'Image Slice {index+1}')\n",
    "            ax[2*i + 1].imshow(img_np[index+1,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        ax[2*i+1].axis('off')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if add_title!='':\n",
    "        print(\"Save Fig\")\n",
    "        plt.savefig(f'{add_title}_class_{selected_class}_patches.png')\n",
    "        plt.close(fig)\n",
    "        \n",
    "    else:\n",
    "        print(\"Plotting..\")\n",
    "        plt.show()\n",
    "    plt.ion()\n",
    "    \n",
    "def reshape_to_square(vector):\n",
    "    # Calculate the nearest square number greater than or equal to the length of the vector\n",
    "    n = int(np.ceil(np.sqrt(len(vector))))\n",
    "    \n",
    "    # Calculate the number of elements to pad with zeros\n",
    "    num_zeros = n*n - len(vector)\n",
    "    \n",
    "    # Pad the vector with zeros if necessary\n",
    "    vector_padded = np.pad(vector, (0, num_zeros), mode='constant')\n",
    "    \n",
    "    # Reshape the padded vector into a square matrix\n",
    "    square_matrix = vector_padded.reshape((n, n))\n",
    "    \n",
    "    return square_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def label_point(x, y, ids, ax):\n",
    "    \"\"\"Annotate points on plot with their IDs.\"\"\"\n",
    "    for i, txt in enumerate(ids):\n",
    "        ax.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "def interactive_plot(x, y, ids=None, colors=None, action='click', img_list=None, emb_list=None, true_img_list=None ,zoom=False, filtered_ids=[]):\n",
    "    \"\"\"Identify the ID of a point by clicking on it.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = [str(i) for i in range(len(x))]\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 8)) if zoom else plt.subplots(1, 4, figsize=(16,4))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    ax[1].axis('off')\n",
    "    ax[2].axis('off')\n",
    "    ax[3].axis('off')\n",
    "    \n",
    "    ax[0].scatter(x, y, s=1, c=colors) if colors is not None else ax[0].scatter(x, y, s=1 if zoom else 0.1)\n",
    "    # Set limit to the plot\n",
    "    if zoom:\n",
    "        ax[4].scatter(x, y, s=1, c=colors) if colors is not None else ax[4].scatter(x, y, s=1)\n",
    "        ax[4].set_xlim([-100, 100])\n",
    "        ax[4].set_ylim([-100, 100])\n",
    "    # Set a threshold based on max values of x and y\n",
    "    threshold = max(max(x) - min(x), max(y) - min(y)) / 80\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    #label_point(x, y, ids, ax[0])\n",
    "    tree = KDTree(np.column_stack((x, y)))\n",
    "    \n",
    "    def onclick(event):\n",
    "        \"\"\"Event handler for mouse click.\"\"\"\n",
    "        if event.inaxes == ax[0]:\n",
    "            dist, i = tree.query([event.xdata, event.ydata])\n",
    "            if dist < threshold:\n",
    "                ax[1].imshow(img_list[int(ids[i])], cmap='gray')\n",
    "                id_img_title = ids[i] if len(filtered_ids) == 0 else filtered_ids[i]\n",
    "                ax[1].set_title(f'Image {id_img_title} (color: {colors[i]})') if colors is not None else ax[1].set_title(f'Image {id_img_title}')\n",
    "                ax[2].imshow(reshape_to_square(emb_list[int(ids[i])]), cmap='gray')\n",
    "                ax[2].set_title(f'Embedding {id_img_title}')\n",
    "                ax[3].imshow(true_img_list[int(ids[i])], cmap='gray')\n",
    "                ax[3].set_title(f'True Image {id_img_title}')\n",
    "                \n",
    "                ax[1].axis('off')\n",
    "                ax[2].axis('off')\n",
    "                ax[3].axis('off')\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    if action == 'click':\n",
    "        fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "    elif action == 'hover':\n",
    "        fig.canvas.mpl_connect('motion_notify_event', onclick)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_2_clusters(embeddings_tsne, cluster_labels, cluster_labels_2d):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=cluster_labels, s=1)\n",
    "    ax[0].set_title('Clustering Results (t-SNE embedding)')\n",
    "\n",
    "    ax[1].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=cluster_labels_2d, s=1)\n",
    "    ax[1].set_title('Clustering Results (t-SNE embedding 2D)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) SIAMESE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/data/falcetta/brain_data\"\n",
    "save_embeddings_path = os.path.join(path_data, f\"embeddings_VDISNET\") \n",
    "\n",
    "dataset_name = 'CAS'\n",
    "\n",
    "save_embeddings_path_SOTA = os.path.join(save_embeddings_path,'SOTA')\n",
    "print(f\"Save embeddings in {save_embeddings_path_SOTA}\")\n",
    "mk_dir(save_embeddings_path_SOTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss, title=''):\n",
    "    plt.figure()\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.title(f\"{title} Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augmentation(img_np):\n",
    "    # Random horizontal flip\n",
    "    if np.random.rand() > 0.3:\n",
    "        img_np = np.flip(img_np, axis=1)\n",
    "    # Random vertical flip\n",
    "    if np.random.rand() > 0.3:\n",
    "        img_np = np.flip(img_np, axis=0)\n",
    "    # Random rotation\n",
    "    if np.random.rand() > 0.3:\n",
    "        img_np = np.rot90(img_np, k=np.random.randint(1, 4))\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define a function to create the transformation matrix\n",
    "def get_transformation_matrix(transform_params):\n",
    "    matrix = np.eye(3)\n",
    "    \n",
    "    if 'flip' in transform_params:\n",
    "        matrix = np.dot(matrix, np.array([[-1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n",
    "    \n",
    "    if 'rotation' in transform_params:\n",
    "        angle = np.deg2rad(transform_params['rotation'])\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(angle), -np.sin(angle), 0],\n",
    "            [np.sin(angle), np.cos(angle), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        matrix = np.dot(matrix, rotation_matrix)\n",
    "    \n",
    "    if 'scale' in transform_params:\n",
    "        scale_matrix = np.array([\n",
    "            [transform_params['scale'], 0, 0],\n",
    "            [0, transform_params['scale'], 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        matrix = np.dot(matrix, scale_matrix)\n",
    "        #print(f\"Scale Matrix: {scale_matrix.shape}\") # 3x3\n",
    "        assert scale_matrix.shape == (3, 3)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Updated AETDataset class\n",
    "class AETDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images_array, transform=None, metadata=None, test_mode=False):\n",
    "        self.images_array = images_array\n",
    "        self.transform = transform\n",
    "        self.test_mode = test_mode\n",
    "        \n",
    "        if metadata is None:\n",
    "            self.compute_metadata()\n",
    "        else:\n",
    "            self.mean, self.std = metadata \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        idx_0 = int(index)\n",
    "        img0 = self.images_array[idx_0]\n",
    "        img0 = Image.fromarray(img0)\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "\n",
    "        # Normalize images\n",
    "        img0 = (img0 - self.mean) / self.std    \n",
    "        # Standardize to 0-1\n",
    "        img0 = (img0 - img0.min()) / (img0.max() - img0.min())\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img0\n",
    "        \n",
    "        # Apply a random transformation to the image and get the transformation matrix\n",
    "        img1, t_true = self.random_augmentation_with_matrix(img0)\n",
    "        \n",
    "        # assert type of img0 and img1 is tensor\n",
    "        assert isinstance(img0, torch.Tensor)\n",
    "        assert isinstance(img1, torch.Tensor)\n",
    "        \n",
    "        #Convert tensor img0 and img1 to float32\n",
    "        img0 = img0.to(torch.float32)\n",
    "        img1 = img1.to(torch.float32)\n",
    "        \n",
    "        return img0, img1, t_true\n",
    "    \n",
    "    def random_augmentation_with_matrix(self, img):\n",
    "        transform_params = {}\n",
    "        original_size = img.size()\n",
    "        #img from tensor to PIL\n",
    "        img = to_pil_image(img)\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            img = FT.hflip(img)\n",
    "            transform_params['flip'] = True\n",
    "        \n",
    "        angle = random.uniform(-30, 30)\n",
    "        img = FT.rotate(img, angle)\n",
    "        transform_params['rotation'] = angle\n",
    "        \n",
    "        scale_factor = random.uniform(0.8, 1.0)\n",
    "        img = FT.resize(img, [int(img.size[0] * scale_factor), int(img.size[1] * scale_factor)])\n",
    "        transform_params['scale'] = scale_factor\n",
    "        \n",
    "        # Resize the image back to its original size\n",
    "        img = FT.resize(img, original_size[1:])\n",
    "        img = FT.to_tensor(img)\n",
    "        \n",
    "        t_true = get_transformation_matrix(transform_params)\n",
    "        t_true = torch.tensor(t_true, dtype=torch.float32).view(-1)  # Flatten to match earlier implementation shape\n",
    "        \n",
    "        return img, t_true\n",
    "        \n",
    "    def compute_metadata(self):\n",
    "        flattened_data = self.images_array.reshape(self.images_array.shape[0], -1)\n",
    "        means = np.mean(flattened_data, axis=0)\n",
    "        stds = np.std(flattened_data, axis=0)\n",
    "        \n",
    "        # Compute the single mean and std\n",
    "        mean = np.mean(means)\n",
    "        std = np.mean(stds)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "        \n",
    "        print(f\"Mean: {self.mean} - Std: {self.std}\")\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return self.mean, self.std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split_arrays(*arrays, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split numpy arrays along the first axis into random train and test subsets.\n",
    "\n",
    "    Parameters:\n",
    "    *arrays : array-like\n",
    "        Arrays to be split. All arrays must have the same size along the first axis.\n",
    "    test_size : float or int, default=0.2\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\n",
    "        If int, represents the absolute number of test samples.\n",
    "    random_state : int or RandomState instance, default=None\n",
    "        Controls the randomness of the training and testing indices.\n",
    "\n",
    "    Returns:\n",
    "    tuple of arrays\n",
    "        Tuple containing train-test split of input arrays.\n",
    "    \"\"\"\n",
    "    # Check if all arrays have the same size along the first axis\n",
    "    first_axis_lengths = [arr.shape[0] for arr in arrays]\n",
    "    if len(set(first_axis_lengths)) != 1:\n",
    "        raise ValueError(\"All input arrays must have the same size along the first axis.\")\n",
    "\n",
    "    assert first_axis_lengths[0] > 0, \"The size of the first axis should be greater than 0.\"\n",
    "    \n",
    "    # Determine the size of the test set\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(test_size * first_axis_lengths[0])\n",
    "    elif isinstance(test_size, int):\n",
    "        if test_size < 0 or test_size > first_axis_lengths[0]:\n",
    "            raise ValueError(\"test_size should be a positive integer less than or equal to the size of the first axis.\")\n",
    "    else:\n",
    "        raise ValueError(\"test_size should be either float or int.\")\n",
    "    \n",
    "    test_size = test_size if test_size > 0 else 1\n",
    "    \n",
    "    # Generate random indices for the test set\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    indices = np.arange(first_axis_lengths[0])\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    # Split arrays\n",
    "    train_arrays = tuple(arr[train_indices] for arr in arrays)\n",
    "    test_arrays = tuple(arr[test_indices] for arr in arrays)\n",
    "\n",
    "    return train_arrays, test_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CAS = np.load(os.path.join(save_embeddings_path,f'X_test_CAS_all.npy'))\n",
    "X_CAS_mask = np.load(os.path.join(save_embeddings_path,f'X_test_mask_CAS_all.npy'))\n",
    "\n",
    "X_CAS_empty = np.load(os.path.join(save_embeddings_path,f'X_test_empty_CAS_all.npy'))\n",
    "X_CAS_mask_empty = np.load(os.path.join(save_embeddings_path,f'X_test_empty_mask_CAS_all.npy'))\n",
    "\n",
    "print(f\"Data loaded\")\n",
    "\n",
    "print(f\"X_test shape: {X_CAS.shape}\")\n",
    "print(f\"X_test_mask shape: {X_CAS_mask.shape}\")\n",
    "\n",
    "print(f\"X_test_empty shape: {X_CAS_empty.shape}\")\n",
    "print(f\"X_test_empty_mask shape: {X_CAS_mask_empty.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CAS_tot = np.concatenate((X_CAS, X_CAS_empty), axis=0)\n",
    "X_CAS_mask_tot = np.concatenate((X_CAS_mask, X_CAS_mask_empty), axis=0)\n",
    "\n",
    "print(f\"X_test_tot shape: {X_CAS_tot.shape}\")\n",
    "print(f\"X_test_mask_tot shape: {X_CAS_mask_tot.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CAS = X_CAS_tot\n",
    "X_CAS_mask = X_CAS_mask_tot\n",
    "\n",
    "print(f\"X_test shape: {X_CAS.shape}\")\n",
    "print(f\"X_test_mask shape: {X_CAS_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(save_embeddings_path_SOTA,f'X_train_CAS_SOTA.npy'))\n",
    "X_train_mask = np.load(os.path.join(save_embeddings_path_SOTA,f'X_train_mask_CAS_SOTA.npy'))\n",
    "\n",
    "X_val = np.load(os.path.join(save_embeddings_path_SOTA,f'X_val_CAS_SOTA.npy'))\n",
    "X_val_mask = np.load(os.path.join(save_embeddings_path_SOTA,f'X_val_mask_CAS_SOTA.npy'))\n",
    "\n",
    "X_train.shape, X_train_mask.shape, X_val.shape, X_val_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    siamese_dataset = AETDataset(images_array=X_train,\n",
    "                                            transform=transforms.ToTensor(),)\n",
    "\n",
    "    metadata = siamese_dataset.get_metadata()\n",
    "    np.save(os.path.join(save_embeddings_path,f'metadata_CAS.npy'), metadata)\n",
    "\n",
    "    siamese_val_dataset = AETDataset(images_array=X_val,\n",
    "                                                transform=transforms.ToTensor(),\n",
    "                                                metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = np.load(os.path.join(save_embeddings_path,f'metadata_CAS.npy'))\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "if not just_sampling:\n",
    "    vis_dataloader = DataLoader(siamese_dataset,\n",
    "                            shuffle=True,\n",
    "                            num_workers=0,\n",
    "                            batch_size=1)\n",
    "\n",
    "    print(f\"Plotting examples\")\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.figure()\n",
    "    for i,example_batch in enumerate(vis_dataloader):\n",
    "        print(example_batch[0].shape, example_batch[1].shape, example_batch[2].shape)\n",
    "        \n",
    "        concatenated = torch.cat((example_batch[0],example_batch[1]),0) # 8,1,100,100\n",
    "        imshow(torchvision.utils.make_grid(concatenated))\n",
    "        if i==3:\n",
    "            break\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class Mapper:\n",
    "    def __init__(self):\n",
    "        # Define the sequence of transformations to apply\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "            transforms.RandomRotation(30),     # Randomly rotate the image by up to 30 degrees\n",
    "            transforms.RandomResizedCrop(32, scale=(0.8, 1.0))  # Randomly crop and resize to 32x32\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Apply the defined transformations to each image in the batch.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): A batch of images with shape (batch_size, 1, 32, 32).\n",
    "\n",
    "        Returns:\n",
    "        tuple: Transformed batch of images and their corresponding transformation matrices.\n",
    "        \"\"\"\n",
    "        transformed_images = []\n",
    "        transformation_matrices = []\n",
    "\n",
    "        for i in range(x.size(0)):  # Iterate over the batch dimension\n",
    "            img = FT.to_pil_image(x[i].cpu())  # Convert tensor to PIL Image\n",
    "\n",
    "            # Initialize transformation matrix as an identity matrix\n",
    "            matrix = np.eye(3)\n",
    "\n",
    "            # Apply Random Horizontal Flip\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                img = FT.hflip(img)\n",
    "                matrix = np.dot(matrix, np.array([[-1, 0, 32], [0, 1, 0], [0, 0, 1]]))  # Horizontal flip matrix\n",
    "\n",
    "            # Apply Random Rotation\n",
    "            angle = torch.empty(1).uniform_(-30, 30).item()\n",
    "            img = FT.rotate(img, angle)\n",
    "            rad = np.deg2rad(angle)\n",
    "            rotation_matrix = np.array([\n",
    "                [np.cos(rad), -np.sin(rad), 0],\n",
    "                [np.sin(rad), np.cos(rad), 0],\n",
    "                [0, 0, 1]\n",
    "            ])\n",
    "            matrix = np.dot(matrix, rotation_matrix)\n",
    "\n",
    "            # Apply Random Resized Crop\n",
    "            i, j, h, w = transforms.RandomResizedCrop.get_params(img, scale=(0.8, 1.0), ratio=(1.0, 1.0))\n",
    "            img = FT.resized_crop(img, i, j, h, w, (32, 32))\n",
    "            crop_matrix = np.array([\n",
    "                [32/w, 0, -32*j/w],\n",
    "                [0, 32/h, -32*i/h],\n",
    "                [0, 0, 1]\n",
    "            ])\n",
    "            matrix = np.dot(matrix, crop_matrix)\n",
    "\n",
    "            img = FT.to_tensor(img)  # Convert back to tensor\n",
    "            transformed_images.append(img)\n",
    "            transformation_matrices.append(matrix)\n",
    "            \n",
    "        #flatten the list of transformation matrices\n",
    "        transformation_matrices = [matrix.flatten() for matrix in transformation_matrices]\n",
    "        #convert to float32\n",
    "        transformation_matrices = np.array(transformation_matrices, dtype=np.float32)\n",
    "\n",
    "        return torch.stack(transformed_images).cuda(), torch.tensor(transformation_matrices).cuda()\n",
    "\n",
    "# Example usage:\n",
    "# mapper = Mapper()\n",
    "# transformed_batch, transformation_matrices = mapper(input_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(256*4*4, latent_dim)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(f\"1- shape: {x.shape}\") # bs, 1, 32, 32\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(f\"2- shape: {x.shape}\") # bs, 64, 32, 32\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"3- shape: {x.shape}\") # bs, 64, 16, 16\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(f\"4- shape: {x.shape}\") # bs, 128, 16, 16\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"5- shape: {x.shape}\") # bs, 128, 8, 8\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(f\"6- shape: {x.shape}\") # bs, 256, 8, 8\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        #print(f\"7- shape: {x.shape}\") # bs, 256, 4, 4\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(f\"8- shape: {x.shape}\") # bs, 256*4*4=4096\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super(Decoder, self).__init__()\n",
    "        n=latent_dim\n",
    "        self.fc1 = nn.Linear(2*n, n)\n",
    "        self.fc2 = nn.Linear(n, n//2)\n",
    "        self.fc3 = nn.Linear(n//2, 3*3)  # Assuming the transformation matrix is 3x3\n",
    "    \n",
    "    def forward(self, z, z_t):\n",
    "        x = torch.cat((z, z_t), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the full AET Model\n",
    "class AETModel(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super(AETModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        self.mapper = Mapper()\n",
    "    \n",
    "    def forward(self, x, return_latent=False):\n",
    "        z = self.encoder(x)\n",
    "        if return_latent:\n",
    "            return z\n",
    "        x_t, true_t = self.mapper(x)\n",
    "        z_t = self.encoder(x_t)\n",
    "        t_pred = self.decoder(z, z_t)\n",
    "        return t_pred, x_t, true_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformation loss function for AET model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformationLoss, self).__init__()\n",
    "        self.loss_accumulator = 0.0\n",
    "        self.num_samples = 0\n",
    "\n",
    "    def forward(self, t_pred, t_true):\n",
    "        # Mean Squared Error loss\n",
    "        loss_transformation = F.mse_loss(t_pred, t_true)\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        self.loss_accumulator += loss_transformation.item()\n",
    "        self.num_samples += 1\n",
    "        return loss_transformation\n",
    "\n",
    "    def get_accumulated_loss(self):\n",
    "        return self.loss_accumulator\n",
    "\n",
    "    def get_mean_loss(self):\n",
    "        if self.num_samples == 0:\n",
    "            return 0.0\n",
    "        mean_loss = self.loss_accumulator / self.num_samples\n",
    "        self.reset()\n",
    "        return mean_loss\n",
    "        \n",
    "    def reset(self):\n",
    "        self.loss_accumulator = 0.0\n",
    "        self.num_samples = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    train_dataloader = DataLoader(siamese_dataset,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8,\n",
    "                            batch_size=128)\n",
    "\n",
    "    val_dataloader = DataLoader(siamese_val_dataset,\n",
    "                            shuffle=False,\n",
    "                            num_workers=8,\n",
    "                            batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "start_epoch=0\n",
    "latent_dim=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    net = AETModel(latent_dim).cuda()\n",
    "\n",
    "    if load_model:\n",
    "        checkpoint_path = os.path.join(save_embeddings_path_SOTA, 'best_model_AET.pt')\n",
    "        print(f\"Loading model from {checkpoint_path}\")\n",
    "        # Load state dictionary into model\n",
    "        net.load_state_dict(torch.load(checkpoint_path))\n",
    "        start_epoch= 0\n",
    "\n",
    "\n",
    "    criterion = TransformationLoss()\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_loss_epoch = 0\n",
    "    cumul_epochs = 0\n",
    "\n",
    "    mean_loss_contrastive_list = []\n",
    "    best_loss_contrastive_list = []\n",
    "    validation_loss_list = []\n",
    "    continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "if not just_sampling:\n",
    "    n_epochs = 500\n",
    "    early_stopping_tolerance = 50\n",
    "    optimizer = optim.Adam(net.parameters(),lr = 0.05)\n",
    "\n",
    "    if continue_training:\n",
    "        mean_loss_contrastive_list = mean_loss_contrastive_list.tolist()\n",
    "        best_loss_contrastive_list = best_loss_contrastive_list.tolist()\n",
    "        validation_loss_list = validation_loss_list.tolist()\n",
    "\n",
    "    print(f\"Starting round of training from epoch {cumul_epochs}\")\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, n_epochs), desc='Epochs'): \n",
    "    # Training loop\n",
    "        net.train()\n",
    "        for data in tqdm(train_dataloader, desc='Batches', leave=False):\n",
    "            img0, img1, matrix = data\n",
    "            img0, img1, matrix = img0.cuda(), img1.cuda(), matrix.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            t_pred, x_t, matrix = net(img0)\n",
    "            #print(f\"t_pred shape: {t_pred.shape}, x_t shape: {x_t.shape}, matrix shape: {matrix.shape}\")\n",
    "            loss_transformation = criterion(t_pred, matrix)\n",
    "            loss_transformation.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate mean loss for transformation loss during training\n",
    "        mean_loss_transformation = criterion.get_mean_loss()\n",
    "        mean_loss_contrastive_list.append(mean_loss_transformation)\n",
    "        \n",
    "        # Validation loop\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(val_dataloader, desc='Validation', leave=False):\n",
    "                val_img0, val_img1, val_matrix = val_data\n",
    "                val_img0, val_img1, val_matrix = val_img0.cuda(), val_img1.cuda(), val_matrix.cuda()\n",
    "                val_t_pred, val_x_t, val_matrix = net(val_img0)\n",
    "                val_loss += criterion(val_t_pred, val_matrix).item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        validation_loss_list.append(val_loss)\n",
    "\n",
    "        # Check if current loss is the best so far\n",
    "        if val_loss < best_loss:\n",
    "            print(f\"Epoch number {cumul_epochs} --- Best loss {val_loss}\")\n",
    "            best_loss = val_loss\n",
    "            best_loss_epoch = cumul_epochs\n",
    "            torch.save(net.state_dict(), os.path.join(save_embeddings_path_SOTA, 'best_model_AET.pt'))\n",
    "        else:\n",
    "            if cumul_epochs - best_loss_epoch > early_stopping_tolerance:\n",
    "                print(f\"Early stopping at epoch {cumul_epochs}\")\n",
    "                best_loss_contrastive_list.append(best_loss)\n",
    "                cumul_epochs += 1\n",
    "                break    \n",
    "\n",
    "        best_loss_contrastive_list.append(best_loss)\n",
    "        cumul_epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'mean_loss_contrastive_list_CAS_AET.npy'), mean_loss_contrastive_list)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'best_loss_contrastive_list_CAS_AET.npy'), best_loss_contrastive_list)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'validation_loss_list_CAS_AET.npy'), validation_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_loss_contrastive_list = np.load(os.path.join(save_embeddings_path_SOTA, f'mean_loss_contrastive_list_CAS_AET.npy'))\n",
    "best_loss_contrastive_list = np.load(os.path.join(save_embeddings_path_SOTA, f'best_loss_contrastive_list_CAS_AET.npy'))\n",
    "validation_loss_list = np.load(os.path.join(save_embeddings_path_SOTA, f'validation_loss_list_CAS_AET.npy'))\n",
    "\n",
    "continue_training=True\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "plt.close('all')\n",
    "show_plot(range(0,len(mean_loss_contrastive_list)),mean_loss_contrastive_list, title='Training Loss')\n",
    "show_plot(range(0,len(best_loss_contrastive_list)),validation_loss_list, title='Val Loss')\n",
    "show_plot(range(0,len(best_loss_contrastive_list)),best_loss_contrastive_list, title='Best Val Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL and Test (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# LOAD BEST MODEL\n",
    "if not just_sampling:\n",
    "    net = AETModel(latent_dim).cuda()\n",
    "    checkpoint_path = os.path.join(save_embeddings_path_SOTA, 'best_model_AET.pt')\n",
    "\n",
    "    # Load state dictionary into model\n",
    "    net.load_state_dict(torch.load(checkpoint_path))\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "from tqdm import tqdm\n",
    "\n",
    "if not just_sampling:\n",
    "    siamese_testset = AETDataset(images_array=X_CAS,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            test_mode=True,\n",
    "                                            metadata=metadata)\n",
    "    \n",
    "    test_dataloader = DataLoader(siamese_testset,\n",
    "                            shuffle=False,\n",
    "                            batch_size=1)\n",
    "\n",
    "\n",
    "    # Get the total number of images\n",
    "    num_images = len(siamese_testset)\n",
    "\n",
    "    # Assuming output1 has a fixed size, get the latent vector size\n",
    "    # Here, we get a dummy batch to determine the size. Replace this with actual size if known\n",
    "    dummy_img = next(iter(test_dataloader)).cuda()\n",
    "    latent_vector_size = net(dummy_img, return_latent=True).cpu().detach().numpy().shape\n",
    "    print(f\"Latent vector size: {latent_vector_size}\")\n",
    "\n",
    "    # Preallocate the numpy array for embeddings\n",
    "    img_embeddings = np.empty((num_images, latent_vector_size[1]))\n",
    "\n",
    "    # Process each image and store the embeddings\n",
    "    for i, img in enumerate(tqdm(test_dataloader)):\n",
    "        img0 = img.cuda()\n",
    "        output1 = net(img0, return_latent=True).cpu().detach().numpy()\n",
    "        img_embeddings[i] = output1.flatten()\n",
    "    \n",
    "\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'img_embeddings_CAS_AET.npy'), img_embeddings)\n",
    "    print(f\"Image Embeddings saved (shape {img_embeddings.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load img embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = np.load(os.path.join(save_embeddings_path_SOTA, f'img_embeddings_CAS_AET.npy'))\n",
    "\n",
    "print(f\"img_embeddings loaded from {save_embeddings_path_SOTA}\")\n",
    "print(f'img_embeddings shape: {img_embeddings.shape}')\n",
    "\n",
    "#Convert as float32\n",
    "print(f\"Unique coordinates: {len(np.unique(img_embeddings, axis=0))} (Over {img_embeddings.shape[0]} total points)\")\n",
    "img_embeddings = img_embeddings.astype(np.float32)\n",
    "print(f\"Unique coordinates: {len(np.unique(img_embeddings, axis=0))} (Over {img_embeddings.shape[0]} total points)\")\n",
    "print(f'img_embeddings type: {type(img_embeddings)}, dtype: {img_embeddings.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_vectors(array):\n",
    "    \"\"\"\n",
    "    Selects two random 128-dimensional vectors from the given array and plots them side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    array (np.ndarray): A NumPy array of shape (n, 128).\n",
    "    \"\"\"\n",
    "    # Ensure the array has the correct shape\n",
    "    if array.shape[1] != 128:\n",
    "        raise ValueError(\"The input array must have 128 columns.\")\n",
    "    \n",
    "    # Select 2 random indices\n",
    "    indices = np.random.choice(array.shape[0], size=2, replace=False)\n",
    "\n",
    "    # Extract the 2 vectors\n",
    "    vector1 = array[indices[0]]\n",
    "    vector2 = array[indices[1]]\n",
    "\n",
    "    # Plot the vectors side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axs[0].plot(vector1)\n",
    "    axs[0].set_title(f'Vector 1 (Index: {indices[0]})')\n",
    "\n",
    "    axs[1].plot(vector2)\n",
    "    axs[1].set_title(f'Vector 2 (Index: {indices[1]})')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_random_vectors(img_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLot IMG Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert img_embeddings[0,:].all() == img_embeddings[1,:].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not just_sampling:\n",
    "    # Perform t-SNE clustering\n",
    "    #Peplexity 500\n",
    "    #early_exaggeration=40\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    img_embeddings_tsne = tsne.fit_transform(img_embeddings)\n",
    "\n",
    "    # save the embeddings\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_AET.npy'), img_embeddings_tsne)\n",
    "    print(f\"TSNE img_embeddings saved in {os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_AET.npy')}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings_tsne = np.load(os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_AET.npy'))\n",
    "print(f\"TSNE img_embeddings loaded\")\n",
    "print(f\"TSNE img_embeddings shape: {img_embeddings_tsne.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique coordinates: {len(np.unique(img_embeddings_tsne, axis=0))} (Over {img_embeddings_tsne.shape[0]} total points)\")\n",
    "print(f\"Percentage of unique coordinates: {len(np.unique(img_embeddings_tsne, axis=0))/img_embeddings_tsne.shape[0]*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    plt.figure()\n",
    "    plt.scatter(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], s=1)\n",
    "    plt.title('Image Clustering Results (t-SNE img_embedding)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=None, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_patches_overlap(X_CAS_mask, X_CAS, indexes=[5597,24,25,26,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_img = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS T-SNE (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "if not just_sampling:\n",
    "    # Create a KMeans object with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters_img, random_state=42, n_init='auto')\n",
    "\n",
    "    # Fit the KMeans model to the img_embeddings\n",
    "    kmeans.fit(img_embeddings_tsne)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    img_cluster_labels_2d = kmeans.labels_\n",
    "    print(f\"Classes: {set(img_cluster_labels_2d)}\")\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_2d_CAS_AET.npy'), img_cluster_labels_2d)\n",
    "\n",
    "\n",
    "    # Plot histogram of cluster labels\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(img_cluster_labels_2d)\n",
    "    plt.xlabel('Cluster Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Cluster Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cluster_labels_2d = np.load(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_2d_CAS_AET.npy'))\n",
    "print(f\"Cluster labels loaded\")\n",
    "print(f\"Cluster labels shape: {img_cluster_labels_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=img_cluster_labels_2d, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS with embedding code (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "if not just_sampling:\n",
    "    # Create a KMeans object with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters_img, random_state=42, n_init='auto')\n",
    "\n",
    "    # Fit the KMeans model to the img_embeddings\n",
    "    kmeans.fit(img_embeddings)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    img_cluster_labels = kmeans.labels_\n",
    "    print(f\"Classes: {set(img_cluster_labels)}\")\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_CAS_AET.npy'), img_cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cluster_labels = np.load(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_CAS_AET.npy'))\n",
    "print(f\"Cluster labels loaded\")\n",
    "print(f\"Cluster labels shape: {img_cluster_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_clusters(img_embeddings_tsne, img_cluster_labels, img_cluster_labels_2d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling: no more from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def calculate_density(features, k=5):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(features)\n",
    "    distances, _ = nbrs.kneighbors(features)\n",
    "    densities = np.sum(distances, axis=1)\n",
    "    return densities\n",
    "\n",
    "def select_representative_samples(densities, num_samples):\n",
    "    print(f\"Selecting {num_samples} samples from {len(densities)} total samples\")\n",
    "    indices = np.argsort(densities)[:num_samples]\n",
    "    return indices\n",
    "\n",
    "def one_shot_AET(features, num_samples, k=5):\n",
    "    \n",
    "    if num_samples < 1:\n",
    "        print(f\"WARNING: num_samples parameter is <1 ({num_samples})\")\n",
    "        print(f\"Taking the {num_samples*100}% of the num_samples ({len(features)}) ==> {int(num_samples * len(features))}\")\n",
    "        num_samples = int(num_samples * len(features))\n",
    "\n",
    "    # Normalize features\n",
    "    features = normalize(features)\n",
    "    \n",
    "    # Step 2: Calculate density of samples in feature space\n",
    "    densities = calculate_density(features, k)\n",
    "    \n",
    "    # Step 3: Select samples with higher local density\n",
    "    selected_indices = select_representative_samples(densities, num_samples)\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "def filter_by_index(idx, *arrays):\n",
    "    return (arr[idx] for arr in arrays)\n",
    "\n",
    "def AET_sampling(features, *embedding_lists, n_size=1000, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    selected_indices = one_shot_AET(features, n_size)\n",
    "    return filter_by_index(selected_indices, *embedding_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_size = 5/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_AET, X_test_mask_AET, img_embeddings_AET, img_embeddings_tsne_AET, img_cluster_labels_AET, img_cluster_labels_2d_AET = AET_sampling(img_embeddings, X_CAS, X_CAS_mask, img_embeddings, img_embeddings_tsne, img_cluster_labels, img_cluster_labels_2d, n_size=n_size, random_seed=42)\n",
    "\n",
    "print(f\"Selected samples filtered\")\n",
    "print(f\"X_test_AET shape: {X_test_AET.shape}\")\n",
    "print(f\"X_test_mask_AET shape: {X_test_mask_AET.shape}\")\n",
    "print(f\"img_embeddings_AET shape: {img_embeddings_AET.shape}\")\n",
    "print(f\"img_embeddings_tsne_AET shape: {img_embeddings_tsne_AET.shape}\")\n",
    "print(f\"img_cluster_labels_AET shape: {img_cluster_labels_AET.shape}\")\n",
    "print(f\"img_cluster_labels_2d_AET shape: {img_cluster_labels_2d_AET.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
