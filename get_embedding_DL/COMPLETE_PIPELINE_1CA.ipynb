{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import set_random_seed, mk_dir\n",
    "from importlib import import_module\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_vessels_np2(dataset_img_dir):\n",
    "    for i,img_name in enumerate(tqdm(os.listdir(dataset_img_dir))):\n",
    "        if '32_img.npy' not in img_name:\n",
    "            continue\n",
    "    # load png as a numpy\n",
    "        img_array = np.load(os.path.join(dataset_img_dir,img_name))\n",
    "        data_array = np.load(os.path.join(dataset_img_dir,img_name).replace('img','label'))\n",
    "        if i==0:\n",
    "            img_list_np = img_array\n",
    "            data_list_np = data_array\n",
    "        else:\n",
    "            img_list_np = np.concatenate((img_list_np, img_array), axis=0)\n",
    "            data_list_np = np.concatenate((data_list_np, data_array), axis=0)\n",
    "\n",
    "    return img_list_np, data_list_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion() \n",
    "\n",
    "def save_img(tensor, name, norm, n_rows=16, scale_each=False):\n",
    "    save_image(tensor, name, nrow=n_rows, padding=5, normalize=norm, pad_value=1, scale_each=scale_each)\n",
    "    \n",
    "def plot_10_patches(img_np, true_vessel_np=False, indexes=None):\n",
    "    \n",
    "    random_indexes = np.random.randint(0, img_np.shape[0], size=5) if indexes is None else indexes\n",
    "\n",
    "    n = len(random_indexes)\n",
    "    \n",
    "    fig, ax = plt.subplots(n, 2, figsize=(6, 3*n))\n",
    "    \n",
    "    for i, index in enumerate(random_indexes):\n",
    "        \n",
    "        if index == img_np.shape[0]:\n",
    "            index -= 1\n",
    "        if index == img_np.shape[0]-1:\n",
    "            index -= 2\n",
    "            \n",
    "        #print(f\"Shape: {img_np[index,:,:].shape}, Max: {img_np[index,:,:].max()}, Min: {img_np[index,:,:].min()}\")\n",
    "        ax[i, 0].set_title(f'True Vessel Slice {index}')\n",
    "        ax[i, 0].imshow(img_np[index,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        ax[i,0].axis('off')\n",
    "        \n",
    "        if true_vessel_np is not False:\n",
    "            print(f\"Shape: {true_vessel_np[index,:,:].shape}, Max: {true_vessel_np[index,:,:].max()}, Min: {true_vessel_np[index,:,:].min()}\")\n",
    "            ax[i, 1].imshow(true_vessel_np[index,:,:], cmap='gray')\n",
    "            ax[i, 1].set_title(f'Image Slice {index}')\n",
    "        else:\n",
    "            ax[i, 1].set_title(f'Image Slice {index+1}')\n",
    "            ax[i, 1].imshow(img_np[index+1,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        ax[i,1].axis('off')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_n_patches_overlap(img_np, true_vessel_np=False, indexes=None, selected_class=None, add_title='', m=5, alpha=0.5, save_dir='clusters_imgs'):\n",
    "    \n",
    "    save_dir = os.path.join(save_embeddings_path,save_dir)\n",
    "    mk_dir(save_dir)\n",
    "    plt.ioff()\n",
    "    n = len(indexes) if indexes is not None else 5\n",
    "    # m is the number of images to plot in each row (2m is the number of columns)\n",
    "    n_rows = int(np.ceil(n/m))\n",
    "    n_cols = m\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, index in enumerate(indexes):\n",
    "        ax[i].set_title(f'Overlay for Slice {index}')\n",
    "        masked_image = np.ma.masked_where(img_np[index,:,:] == 0, true_vessel_np[index,:,:])\n",
    "        # Overlay the red image on top of true_vessel_np[index,:,:]\n",
    "        ax[i].imshow(true_vessel_np[index,:,:], cmap='gray', interpolation='none')\n",
    "        ax[i].imshow(masked_image, cmap='Reds', alpha=alpha)\n",
    "        \n",
    "        ax[i].axis('off')\n",
    "    \n",
    "    try:\n",
    "        plt.tight_layout()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if add_title!='':\n",
    "        print(f\"Save Fig to {save_dir}\")\n",
    "        plt.savefig(os.path.join(save_dir,f'{add_title}_class_{selected_class}_patches.png'))\n",
    "        plt.close(fig)\n",
    "        \n",
    "    else:\n",
    "        print(\"Plotting..\")\n",
    "        plt.show()\n",
    "    plt.ion()\n",
    "\n",
    "    \n",
    "def plot_n_patches(img_np, true_vessel_np=False, indexes=None, selected_class=None, add_title='', m=5):\n",
    "    plt.ioff()\n",
    "    n = len(indexes) if indexes is not None else 5\n",
    "    if n > 1000:\n",
    "        print(f'WARNING: YOU ARE TRYING TO PLOT {n} images')\n",
    "    # m is the number of images to plot in each row (2m is the number of columns)\n",
    "    n_rows = int(np.ceil(n/m))\n",
    "    n_cols = 2*m\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i, index in enumerate(indexes):\n",
    "        \n",
    "        if index == img_np.shape[0]:\n",
    "            index -= 1\n",
    "        if index == img_np.shape[0]-1:\n",
    "            index -= 2\n",
    "            \n",
    "        ax[2*i].set_title(f'Image Slice {index}')\n",
    "        ax[2*i].imshow(img_np[index,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        ax[2*i].axis('off')\n",
    "        print(f\"Shape: {img_np[index,:,:].shape}, Max: {img_np[index,:,:].max()}, Min: {img_np[index,:,:].min()}\")\n",
    "        if true_vessel_np is not False:\n",
    "            ax[2*i + 1].imshow(true_vessel_np[index,:,:], cmap='gray')\n",
    "            ax[2*i + 1].set_title(f'True Vessel Slice {index}')\n",
    "            \n",
    "        else:\n",
    "            ax[2*i + 1].set_title(f'Image Slice {index+1}')\n",
    "            ax[2*i + 1].imshow(img_np[index+1,:,:], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        ax[2*i+1].axis('off')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if add_title!='':\n",
    "        print(\"Save Fig\")\n",
    "        plt.savefig(f'{add_title}_class_{selected_class}_patches.png')\n",
    "        plt.close(fig)\n",
    "        \n",
    "    else:\n",
    "        print(\"Plotting..\")\n",
    "        plt.show()\n",
    "    plt.ion()\n",
    "    \n",
    "def reshape_to_square(vector):\n",
    "    # Calculate the nearest square number greater than or equal to the length of the vector\n",
    "    n = int(np.ceil(np.sqrt(len(vector))))\n",
    "    \n",
    "    # Calculate the number of elements to pad with zeros\n",
    "    num_zeros = n*n - len(vector)\n",
    "    \n",
    "    # Pad the vector with zeros if necessary\n",
    "    vector_padded = np.pad(vector, (0, num_zeros), mode='constant')\n",
    "    \n",
    "    # Reshape the padded vector into a square matrix\n",
    "    square_matrix = vector_padded.reshape((n, n))\n",
    "    \n",
    "    return square_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_vessels_np2(dataset_img_dir):\n",
    "    for i,img_name in enumerate(tqdm(os.listdir(dataset_img_dir))):\n",
    "        if '32_img.npy' not in img_name:\n",
    "            continue\n",
    "        assert '32_img.npy' in img_name\n",
    "        assert 'label' not in img_name\n",
    "    # load png as a numpy\n",
    "        img_array = np.load(os.path.join(dataset_img_dir,img_name))\n",
    "        data_array = np.load(os.path.join(dataset_img_dir,img_name).replace('img','label'))\n",
    "        if i==0:\n",
    "            img_list_np = img_array\n",
    "            data_list_np = data_array\n",
    "        else:\n",
    "            img_list_np = np.concatenate((img_list_np, img_array), axis=0)\n",
    "            data_list_np = np.concatenate((data_list_np, data_array), axis=0)\n",
    "\n",
    "    return img_list_np, data_list_np\n",
    "\n",
    "def extract_empty_img_vessels(dataset_empty_img_dir):\n",
    "    for i,img_name in enumerate(tqdm(os.listdir(dataset_empty_img_dir))):\n",
    "        if '32_img.npy' not in img_name:\n",
    "            continue\n",
    "    # load png as a numpy\n",
    "        img_array = np.load(os.path.join(dataset_empty_img_dir,img_name))\n",
    "        if i==0:\n",
    "            img_list_np = img_array\n",
    "        else:\n",
    "            img_list_np = np.concatenate((img_list_np, img_array), axis=0)\n",
    "\n",
    "    return img_list_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def label_point(x, y, ids, ax):\n",
    "    \"\"\"Annotate points on plot with their IDs.\"\"\"\n",
    "    for i, txt in enumerate(ids):\n",
    "        ax.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "def interactive_plot(x, y, ids=None, colors=None, action='click', img_list=None, emb_list=None, true_img_list=None ,zoom=False, filtered_ids=[]):\n",
    "    \"\"\"Identify the ID of a point by clicking on it.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = [str(i) for i in range(len(x))]\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 8)) if zoom else plt.subplots(1, 4, figsize=(16,4))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    ax[1].axis('off')\n",
    "    ax[2].axis('off')\n",
    "    ax[3].axis('off')\n",
    "    \n",
    "    ax[0].scatter(x, y, s=1, c=colors) if colors is not None else ax[0].scatter(x, y, s=1 if zoom else 0.1)\n",
    "    # Set limit to the plot\n",
    "    if zoom:\n",
    "        ax[4].scatter(x, y, s=1, c=colors) if colors is not None else ax[4].scatter(x, y, s=1)\n",
    "        ax[4].set_xlim([-100, 100])\n",
    "        ax[4].set_ylim([-100, 100])\n",
    "    # Set a threshold based on max values of x and y\n",
    "    threshold = max(max(x) - min(x), max(y) - min(y)) / 80\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    #label_point(x, y, ids, ax[0])\n",
    "    tree = KDTree(np.column_stack((x, y)))\n",
    "    \n",
    "    def onclick(event):\n",
    "        \"\"\"Event handler for mouse click.\"\"\"\n",
    "        if event.inaxes == ax[0]:\n",
    "            dist, i = tree.query([event.xdata, event.ydata])\n",
    "            if dist < threshold:\n",
    "                ax[1].imshow(img_list[int(ids[i])], cmap='gray')\n",
    "                id_img_title = ids[i] if len(filtered_ids) == 0 else filtered_ids[i]\n",
    "                ax[1].set_title(f'Image {id_img_title} (color: {colors[i]})') if colors is not None else ax[1].set_title(f'Image {id_img_title}')\n",
    "                ax[2].imshow(reshape_to_square(emb_list[int(ids[i])]), cmap='gray')\n",
    "                ax[2].set_title(f'Embedding {id_img_title}')\n",
    "                ax[3].imshow(true_img_list[int(ids[i])], cmap='gray')\n",
    "                ax[3].set_title(f'True Image {id_img_title}')\n",
    "                \n",
    "                ax[1].axis('off')\n",
    "                ax[2].axis('off')\n",
    "                ax[3].axis('off')\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    if action == 'click':\n",
    "        fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "    elif action == 'hover':\n",
    "        fig.canvas.mpl_connect('motion_notify_event', onclick)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_2_clusters(embeddings_tsne, cluster_labels, cluster_labels_2d):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=cluster_labels, s=1)\n",
    "    ax[0].set_title('Clustering Results (t-SNE embedding)')\n",
    "\n",
    "    ax[1].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=cluster_labels_2d, s=1)\n",
    "    ax[1].set_title('Clustering Results (t-SNE embedding 2D)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) SIAMESE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/data/falcetta/brain_data\"\n",
    "save_embeddings_path = os.path.join(path_data, f\"embeddings_VDISNET\") \n",
    "\n",
    "dataset_name = 'CAS'\n",
    "\n",
    "save_embeddings_path_SOTA = os.path.join(save_embeddings_path,'SOTA')\n",
    "print(f\"Save embeddings in {save_embeddings_path_SOTA}\")\n",
    "mk_dir(save_embeddings_path_SOTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss, title=''):\n",
    "    plt.figure()\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.title(f\"{title} Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augmentation(img_np):\n",
    "    # Random horizontal flip\n",
    "    if np.random.rand() > 0.3:\n",
    "        img_np = np.flip(img_np, axis=1)\n",
    "    # Random vertical flip\n",
    "    if np.random.rand() > 0.3:\n",
    "        img_np = np.flip(img_np, axis=0)\n",
    "    # Random rotation\n",
    "    if np.random.rand() > 0.3:\n",
    "        img_np = np.rot90(img_np, k=np.random.randint(1, 4))\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images_array, transform=None, test_mode=False, mode='double', metadata=None):\n",
    "        self.images_array = images_array\n",
    "        self.transform = transform\n",
    "        \n",
    "        if metadata is None:\n",
    "            self.compute_metadata()\n",
    "        else:\n",
    "            self.mean, self.std = metadata \n",
    "            \n",
    "        self.test_mode = test_mode\n",
    "        self.mode=mode\n",
    "         \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        # Select random index from the dataset\n",
    "        if self.mode == 'single':\n",
    "            idx_0 = int(index)\n",
    "            img0 = self.images_array[idx_0]\n",
    "            img0 = Image.fromarray(img0)\n",
    "            # Apply transformations if provided\n",
    "            if self.transform is not None:\n",
    "                img0 = self.transform(img0)\n",
    "            # Normalize images\n",
    "            img0 = (img0 - self.mean) / self.std    \n",
    "            # Standardize to 0-1\n",
    "            img0 = (img0 - img0.min()) / (img0.max() - img0.min())\n",
    "                \n",
    "            return img0\n",
    "        \n",
    "        else:\n",
    "            idx_0 = int(index)\n",
    "            img0 = self.images_array[idx_0]\n",
    "            \n",
    "            # Determine whether to select a sample from the same class or different class\n",
    "            should_get_same_class = random.randint(0, 1) \n",
    "            if should_get_same_class:\n",
    "                img1 = random_augmentation(img0)\n",
    "            else:\n",
    "                idx_1 = random.randint(0, len(self.images_array) - 1)\n",
    "                img1 = self.images_array[idx_1]\n",
    "                \n",
    "            if self.test_mode:\n",
    "                print(f\"Testing {idx_0} against {idx_1}\")\n",
    "                \n",
    "            # Convert numpy arrays to PIL Images\n",
    "            img0 = Image.fromarray(img0)\n",
    "            img1 = Image.fromarray(img1)\n",
    "    \n",
    "            # Apply transformations if provided\n",
    "            if self.transform is not None:\n",
    "                img0 = self.transform(img0)\n",
    "                img1 = self.transform(img1)\n",
    "            \n",
    "            # Normalize images\n",
    "            img0 = (img0 - self.mean) / self.std\n",
    "            img1 = (img1 - self.mean) / self.std\n",
    "            \n",
    "            # Standardize to 0-1\n",
    "            img0 = (img0 - img0.min()) / (img0.max() - img0.min())\n",
    "            img1 = (img1 - img1.min()) / (img1.max() - img1.min())\n",
    "                \n",
    "            # Flag: 1 if same class, 0 if different class\n",
    "            flag = torch.from_numpy((1-should_get_same_class) * np.ones(1))\n",
    "            \n",
    "            return img0, img1, flag\n",
    "        \n",
    "    def compute_metadata(self):\n",
    "        flattened_data = self.images_array.reshape(self.images_array.shape[0], -1)\n",
    "        means = np.mean(flattened_data, axis=0)\n",
    "        stds = np.std(flattened_data, axis=0)\n",
    "        \n",
    "        # compute the single mean and std\n",
    "        mean = np.mean(means)\n",
    "        std = np.mean(stds)\n",
    "        \n",
    "        #convert to pytorch\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "        \n",
    "        print(f\"Mean: {self.mean} - Std: {self.std}\")\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return self.mean, self.std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split_arrays(*arrays, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split numpy arrays along the first axis into random train and test subsets.\n",
    "\n",
    "    Parameters:\n",
    "    *arrays : array-like\n",
    "        Arrays to be split. All arrays must have the same size along the first axis.\n",
    "    test_size : float or int, default=0.2\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\n",
    "        If int, represents the absolute number of test samples.\n",
    "    random_state : int or RandomState instance, default=None\n",
    "        Controls the randomness of the training and testing indices.\n",
    "\n",
    "    Returns:\n",
    "    tuple of arrays\n",
    "        Tuple containing train-test split of input arrays.\n",
    "    \"\"\"\n",
    "    # Check if all arrays have the same size along the first axis\n",
    "    first_axis_lengths = [arr.shape[0] for arr in arrays]\n",
    "    if len(set(first_axis_lengths)) != 1:\n",
    "        raise ValueError(\"All input arrays must have the same size along the first axis.\")\n",
    "\n",
    "    assert first_axis_lengths[0] > 0, \"The size of the first axis should be greater than 0.\"\n",
    "    \n",
    "    # Determine the size of the test set\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(test_size * first_axis_lengths[0])\n",
    "    elif isinstance(test_size, int):\n",
    "        if test_size < 0 or test_size > first_axis_lengths[0]:\n",
    "            raise ValueError(\"test_size should be a positive integer less than or equal to the size of the first axis.\")\n",
    "    else:\n",
    "        raise ValueError(\"test_size should be either float or int.\")\n",
    "    \n",
    "    test_size = test_size if test_size > 0 else 1\n",
    "    \n",
    "    # Generate random indices for the test set\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    indices = np.arange(first_axis_lengths[0])\n",
    "    rng.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    # Split arrays\n",
    "    train_arrays = tuple(arr[train_indices] for arr in arrays)\n",
    "    test_arrays = tuple(arr[test_indices] for arr in arrays)\n",
    "\n",
    "    return train_arrays, test_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    X_CAS = np.load(os.path.join(save_embeddings_path,f'X_test_CAS_all.npy'))\n",
    "    X_CAS_mask = np.load(os.path.join(save_embeddings_path,f'X_test_mask_CAS_all.npy'))\n",
    "\n",
    "    X_CAS_empty = np.load(os.path.join(save_embeddings_path,f'X_test_empty_CAS_all.npy'))\n",
    "    X_CAS_mask_empty = np.load(os.path.join(save_embeddings_path,f'X_test_empty_mask_CAS_all.npy'))\n",
    "\n",
    "    print(f\"Data loaded\")\n",
    "\n",
    "    print(f\"X_test shape: {X_CAS.shape}\")\n",
    "    print(f\"X_test_mask shape: {X_CAS_mask.shape}\")\n",
    "\n",
    "    print(f\"X_test_empty shape: {X_CAS_empty.shape}\")\n",
    "    print(f\"X_test_empty_mask shape: {X_CAS_mask_empty.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    X_CAS_tot = np.concatenate((X_CAS, X_CAS_empty), axis=0)\n",
    "    X_CAS_mask_tot = np.concatenate((X_CAS_mask, X_CAS_mask_empty), axis=0)\n",
    "\n",
    "    print(f\"X_test_tot shape: {X_CAS_tot.shape}\")\n",
    "    print(f\"X_test_mask_tot shape: {X_CAS_mask_tot.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    X_CAS = X_CAS_tot\n",
    "    X_CAS_mask = X_CAS_mask_tot\n",
    "\n",
    "    print(f\"X_test shape: {X_CAS.shape}\")\n",
    "    print(f\"X_test_mask shape: {X_CAS_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    # Perform train-test split\n",
    "    train_arrays, val_arrays = train_test_split_arrays(X_CAS, X_CAS_mask, test_size=0.15, random_state=42)\n",
    "\n",
    "    X_train, X_train_mask = train_arrays\n",
    "    X_val, X_val_mask = val_arrays\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_train_mask shape: {X_train_mask.shape}\")\n",
    "\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"X_val_mask shape: {X_val_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    \n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'X_train_CAS_SOTA.npy'), X_train)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'X_train_mask_CAS_SOTA.npy'), X_train_mask)\n",
    "\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'X_val_CAS_SOTA.npy'), X_val)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'X_val_mask_CAS_SOTA.npy'), X_val_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(save_embeddings_path_SOTA,f'X_train_CAS_SOTA.npy'))\n",
    "X_train_mask = np.load(os.path.join(save_embeddings_path_SOTA,f'X_train_mask_CAS_SOTA.npy'))\n",
    "\n",
    "X_val = np.load(os.path.join(save_embeddings_path_SOTA,f'X_val_CAS_SOTA.npy'))\n",
    "X_val_mask = np.load(os.path.join(save_embeddings_path_SOTA,f'X_val_mask_CAS_SOTA.npy'))\n",
    "\n",
    "X_train.shape, X_train_mask.shape, X_val.shape, X_val_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    siamese_dataset = SiameseNetworkDataset(images_array=X_train,\n",
    "                                            transform=transforms.ToTensor(),)\n",
    "\n",
    "    metadata = siamese_dataset.get_metadata()\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'metadata_CAS_SOTA.npy'), metadata)\n",
    "\n",
    "    siamese_val_dataset = SiameseNetworkDataset(images_array=X_val,\n",
    "                                                transform=transforms.ToTensor(),\n",
    "                                                metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = np.load(os.path.join(save_embeddings_path_SOTA,f'metadata_CAS_SOTA.npy'))\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "if not just_sampling:\n",
    "    vis_dataloader = DataLoader(siamese_dataset,\n",
    "                            shuffle=True,\n",
    "                            num_workers=0,\n",
    "                            batch_size=1)\n",
    "\n",
    "    plot_same = True\n",
    "    plot_diff = True\n",
    "\n",
    "    print(f\"Plotting examples of the different (0) and same(1) clusters\")\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.figure()\n",
    "    for example_batch in vis_dataloader:\n",
    "        if int(example_batch[2]) == 0 and plot_same:\n",
    "            concatenated = torch.cat((example_batch[0],example_batch[1]),0) # 8,1,100,100\n",
    "            imshow(torchvision.utils.make_grid(concatenated))\n",
    "            print(f\"Flag: {int(example_batch[2])}\")\n",
    "            plot_same = False\n",
    "        if int(example_batch[2]) == 1 and plot_diff:\n",
    "            concatenated = torch.cat((example_batch[0],example_batch[1]),0) # 8,1,100,100\n",
    "            imshow(torchvision.utils.make_grid(concatenated))\n",
    "            print(f\"Flag: {int(example_batch[2])}\")\n",
    "            plot_diff = False\n",
    "        if not plot_same and not plot_diff:\n",
    "            break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size=128, pretrained = False):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet-50\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 1 input channel\n",
    "        self.resnet_features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            *list(resnet.children())[1:-2]\n",
    "        )\n",
    "        # Remove the last fully connected layer of ResNet-50\n",
    "        #self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Define fully connected layers for embedding\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(resnet.fc.in_features, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Linear(1000, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Extract features using ResNet\n",
    "        features = self.resnet_features(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        output = self.fc1(features)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2, mode='double'):\n",
    "        output1 = self.forward_once(input1)\n",
    "        if mode == 'single':\n",
    "            return output1\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.loss_accumulator = 0.0\n",
    "        self.num_samples = 0\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        \n",
    "        self.loss_accumulator += loss_contrastive.item()\n",
    "        self.num_samples += 1\n",
    "        return loss_contrastive\n",
    "\n",
    "    def get_accumulated_loss(self):\n",
    "        return self.loss_accumulator\n",
    "\n",
    "    def get_mean_loss(self):\n",
    "        self.mean_loss = self.loss_accumulator / self.num_samples\n",
    "        self.reset()\n",
    "        return self.mean_loss\n",
    "        \n",
    "    def reset(self):\n",
    "        self.loss_accumulator = 0.0\n",
    "        self.num_samples = 0\n",
    "\n",
    "\"\"\"\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=.5, **kwargs):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        # self.metric = metric\n",
    "        self.distance = torch.nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def forward(self, out0, out1, label):\n",
    "        gt = label.float()\n",
    "        D = self.distance(out0, out1).float().squeeze()\n",
    "        loss = gt * 0.5 * torch.pow(D, 2) + (1 - gt) * 0.5 * torch.pow(torch.clamp(self.margin - D, min=0.0), 2)\n",
    "        return loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    train_dataloader = DataLoader(siamese_dataset,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8,\n",
    "                            batch_size=128)\n",
    "\n",
    "    val_dataloader = DataLoader(siamese_val_dataset,\n",
    "                            shuffle=False,\n",
    "                            num_workers=8,\n",
    "                            batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "start_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    net = SiameseNetwork().cuda()\n",
    "\n",
    "    if load_model:\n",
    "        checkpoint_path = os.path.join(save_embeddings_path_SOTA, 'best_model_CA.pt')\n",
    "        print(f\"Loading model from {checkpoint_path}\")\n",
    "        # Load state dictionary into model\n",
    "        net.load_state_dict(torch.load(checkpoint_path))\n",
    "        start_epoch= 0\n",
    "\n",
    "\n",
    "    criterion = ContrastiveLoss()\n",
    "\n",
    "    best_loss = 9999\n",
    "    best_loss_epoch = 0\n",
    "    cumul_epochs = 0\n",
    "\n",
    "    mean_loss_contrastive_list = []\n",
    "    best_loss_contrastive_list = []\n",
    "    validation_loss_list = []\n",
    "    continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "if not just_sampling:\n",
    "    n_epochs = 500\n",
    "    early_stopping_tolerance = 50\n",
    "    optimizer = optim.Adam(net.parameters(),lr = 0.0005)\n",
    "\n",
    "    if continue_training:\n",
    "        mean_loss_contrastive_list = mean_loss_contrastive_list.tolist()\n",
    "        best_loss_contrastive_list = best_loss_contrastive_list.tolist()\n",
    "        validation_loss_list = validation_loss_list.tolist()\n",
    "\n",
    "    print(f\"Starting round of training from epoch {cumul_epochs}\")\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, n_epochs), desc='Epochs'): \n",
    "        # Training loop\n",
    "        net.train()\n",
    "        for data in tqdm(train_dataloader, desc='Batches', leave=False):\n",
    "            img0, img1, label = data\n",
    "            img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2 = net(img0, img1)\n",
    "            loss_contrastive = criterion(output1, output2, label)\n",
    "            loss_contrastive.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate mean loss for contrastive loss during training\n",
    "        mean_loss_contrastive = criterion.get_mean_loss()\n",
    "        mean_loss_contrastive_list.append(mean_loss_contrastive)\n",
    "        \n",
    "        # Validation loop\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(val_dataloader, desc='Validation', leave=False):\n",
    "                val_img0, val_img1, val_label = val_data\n",
    "                val_img0, val_img1, val_label = val_img0.cuda(), val_img1.cuda(), val_label.cuda()\n",
    "                val_output1, val_output2 = net(val_img0, val_img1)\n",
    "                val_loss += criterion(val_output1, val_output2, val_label).item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        validation_loss_list.append(val_loss)\n",
    "        #print(f\"Validation Loss: {val_loss:.2f}\")\n",
    "\n",
    "        # Check if current loss is the best so far\n",
    "        if val_loss < best_loss:\n",
    "            print(f\"Epoch number {cumul_epochs} --- Best loss {val_loss:.2f}\")\n",
    "            best_loss = val_loss\n",
    "            best_loss_epoch = cumul_epochs\n",
    "            torch.save(net.state_dict(), os.path.join(save_embeddings_path_SOTA, 'best_model_CA.pt'))\n",
    "        else:\n",
    "            if cumul_epochs - best_loss_epoch > early_stopping_tolerance:\n",
    "                print(f\"Early stopping at epoch {cumul_epochs}\")\n",
    "                best_loss_contrastive_list.append(best_loss)\n",
    "                cumul_epochs +=1\n",
    "                break    \n",
    "        \n",
    "        best_loss_contrastive_list.append(best_loss)\n",
    "        cumul_epochs +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'mean_loss_contrastive_list_CAS_CA.npy'), mean_loss_contrastive_list)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'best_loss_contrastive_list_CAS_CA.npy'), best_loss_contrastive_list)\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'validation_loss_list_CAS_CA.npy'), validation_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not just_sampling:\n",
    "    mean_loss_contrastive_list = np.load(os.path.join(save_embeddings_path_SOTA, f'mean_loss_contrastive_list_CAS_CA.npy'))\n",
    "    best_loss_contrastive_list = np.load(os.path.join(save_embeddings_path_SOTA, f'best_loss_contrastive_list_CAS_CA.npy'))\n",
    "    validation_loss_list = np.load(os.path.join(save_embeddings_path_SOTA, f'validation_loss_list_CAS_CA.npy'))\n",
    "\n",
    "    continue_training=True\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.ioff()\n",
    "    plt.close('all')\n",
    "    show_plot(range(0,len(mean_loss_contrastive_list)),mean_loss_contrastive_list, title='Training Loss')\n",
    "    show_plot(range(0,len(best_loss_contrastive_list)),validation_loss_list, title='Val Loss')\n",
    "    show_plot(range(0,len(best_loss_contrastive_list)),best_loss_contrastive_list, title='Best Val Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL and Test (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# LOAD BEST MODEL\n",
    "if not just_sampling:\n",
    "    net = SiameseNetwork().cuda()\n",
    "    checkpoint_path = os.path.join(save_embeddings_path_SOTA, 'best_model_CA.pt')\n",
    "\n",
    "    # Load state dictionary into model\n",
    "    net.load_state_dict(torch.load(checkpoint_path))\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "from tqdm import tqdm\n",
    "\n",
    "if not just_sampling:\n",
    "    batch_size = 64\n",
    "    siamese_testset = SiameseNetworkDataset(images_array=X_CAS,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            test_mode=True,\n",
    "                                            metadata=metadata,\n",
    "                                            mode = 'single')\n",
    "\n",
    "    test_dataloader = DataLoader(siamese_testset,\n",
    "                            shuffle=False,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "    # Pre-allocate memory for the embeddings array\n",
    "    num_samples = len(siamese_testset)\n",
    "    embedding_size = 128  # Assuming the size of the embeddings is 128, adjust as necessary\n",
    "    img_embeddings = np.empty((num_samples, embedding_size))\n",
    "    print(f\"Number of samples: {num_samples} (batch size: {batch_size}) ==> Passages: {num_samples // batch_size + 1}\")\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    for img in tqdm(test_dataloader):\n",
    "        img0 = img.cuda()\n",
    "        output = net(img0, None, mode='single').cpu().detach().numpy()\n",
    "        # Calculate the end index for the current batch\n",
    "        end_idx = start_idx + output.shape[0]\n",
    "        # Store the batch of embeddings in the pre-allocated array\n",
    "        img_embeddings[start_idx:end_idx] = output    \n",
    "        # Update the start index for the next batch\n",
    "        start_idx = end_idx\n",
    "            \n",
    "\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA, f'img_embeddings_CAS_CA.npy'), img_embeddings)\n",
    "    print(f\"Image Embeddings saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load img embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = np.load(os.path.join(save_embeddings_path_SOTA, f'img_embeddings_CAS_CA.npy'))\n",
    "\n",
    "print(f\"img_embeddings loaded from {save_embeddings_path_SOTA}\")\n",
    "print(f'img_embeddings shape: {img_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLot IMG Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not just_sampling:\n",
    "    # Perform t-SNE clustering\n",
    "    #Peplexity 500\n",
    "    #early_exaggeration=40\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    img_embeddings_tsne = tsne.fit_transform(img_embeddings)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], s=1)\n",
    "    plt.title('Image Clustering Results (t-SNE img_embedding)')\n",
    "\n",
    "    # save the embeddings\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_CA.npy'), img_embeddings_tsne)\n",
    "    print(f\"TSNE img_embeddings saved in {os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_CA.npy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings_tsne = np.load(os.path.join(save_embeddings_path_SOTA,f'img_embeddings_tsne_CAS_CA.npy'))\n",
    "print(f\"TSNE img_embeddings loaded\")\n",
    "print(f\"TSNE img_embeddings shape: {img_embeddings_tsne.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=None, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_patches_overlap(X_CAS_mask, X_CAS, indexes=[5597,24,25,26,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_img = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS T-SNE (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "if not just_sampling:\n",
    "    # Create a KMeans object with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters_img, random_state=42, n_init='auto')\n",
    "\n",
    "    # Fit the KMeans model to the img_embeddings\n",
    "    kmeans.fit(img_embeddings_tsne)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    img_cluster_labels_2d = kmeans.labels_\n",
    "    print(f\"Classes: {set(img_cluster_labels_2d)}\")\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_2d_CAS_CA.npy'), img_cluster_labels_2d)\n",
    "\n",
    "\n",
    "    # Plot histogram of cluster labels\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(img_cluster_labels_2d)\n",
    "    plt.xlabel('Cluster Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Cluster Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cluster_labels_2d = np.load(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_2d_CAS_CA.npy'))\n",
    "print(f\"Cluster labels loaded\")\n",
    "print(f\"Cluster labels shape: {img_cluster_labels_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=img_cluster_labels_2d, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS with embedding code (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "if not just_sampling:\n",
    "    # Create a KMeans object with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters_img, random_state=42, n_init='auto')\n",
    "\n",
    "    # Fit the KMeans model to the img_embeddings\n",
    "    kmeans.fit(img_embeddings)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    img_cluster_labels = kmeans.labels_\n",
    "    print(f\"Classes: {set(img_cluster_labels)}\")\n",
    "    np.save(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_CAS_CA.npy'), img_cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cluster_labels = np.load(os.path.join(save_embeddings_path_SOTA,f'img_cluster_labels_CAS_CA.npy'))\n",
    "print(f\"Cluster labels loaded\")\n",
    "print(f\"Cluster labels shape: {img_cluster_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_clusters(img_embeddings_tsne, img_cluster_labels, img_cluster_labels_2d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot(img_embeddings_tsne[:, 0], img_embeddings_tsne[:, 1], colors=img_cluster_labels, action='click', img_list=X_CAS_mask, emb_list=img_embeddings, true_img_list=X_CAS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
