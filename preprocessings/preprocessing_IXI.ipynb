{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"IXI\"\n",
    "DATA_PATH = \"/data/falcetta/brain_data/IXI/volumes\"\n",
    "OUTPUT_PATH = \"/data/falcetta/brain_data/IXIJ/processed\"\n",
    "\n",
    "#these functions are to define for each new dataset, unless fixing a rule\n",
    "#they allow to define a clear association between the data path of a brain image and the corresponding brain/vessel mask\n",
    "#the brain mask is not mandatory, if not present just return None\n",
    "import utils\n",
    "def img_path_to_brain_path(img_path):\n",
    "    out_path =img_path.replace(\"volumes/\",\"masks/\").replace(\".nii.gz\",\"_mask.nii.gz\")\n",
    "    return out_path\n",
    "def img_path_to_vessel_path(img_path):\n",
    "    out_path =img_path.replace(\"volumes/\",\"vessels/\").replace(\".nii.gz\",\"_vessel_mask2.nii.gz\")\n",
    "    return out_path\n",
    "def img_path_to_weight_path(img_path):\n",
    "    out_path =img_path.replace(\".nii.gz\",\"_weight.nii.gz\")\n",
    "    return out_path\n",
    "\n",
    "utils.img_path_to_brain_path = img_path_to_brain_path\n",
    "utils.img_path_to_vessel_path = img_path_to_vessel_path\n",
    "utils.img_path_to_weight_path = img_path_to_weight_path\n",
    "\n",
    "#this regex works as a filter in case you have unnecessary .nii files in your dataset folder\n",
    "PATH_RULE = \"\" #\"^.*MRA.nii.gz$\"\n",
    "\n",
    "#if you have some outliers you want to discard, you can specify it in the list below\n",
    "OUTLIERs = []\n",
    "\n",
    "#if you need to preprocess also images with no vessel masks set the following flag to False\n",
    "VESSELs_REQUIRED = False\n",
    "\n",
    "#this parameter is to unify the orientation of our images\n",
    "#set it True if you notice in the examples shown below that the nose is oriented downward and not upward\n",
    "utils.do_flip = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import search_nii, extract_paths, load_info_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(f\"info_{DATASET_NAME}.pkl\"):\n",
    "    info = {\n",
    "        \"train\": extract_paths(path_rule=PATH_RULE),\n",
    "        \"val\": None,\n",
    "        \"test\": None,\n",
    "    }\n",
    "    search_nii(DATA_PATH, info[\"train\"])\n",
    "else:\n",
    "    info = load_info_from_checkpoint(f\"info_{DATASET_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from utils import print_bold, load_and_display_middle_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_bold(\"[Example 01]\")\n",
    "random.seed(0)\n",
    "load_and_display_middle_slice(random.choice(info[\"train\"].paths), display_header=True)\n",
    "\n",
    "print_bold(\"[Example 02]\")\n",
    "random.seed(1)\n",
    "load_and_display_middle_slice(random.choice(info[\"train\"].paths), axis=[0,1,2], display_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_medical_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGs = sum([\n",
    "    info[\"train\"].paths,\n",
    "    info[\"val\"].paths if info[\"val\"] is not None else [],\n",
    "    info[\"test\"].paths if info[\"test\"] is not None else []\n",
    "], [])\n",
    "\n",
    "len(IMGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volumes. First, we discard the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGs = [img_path for img_path in IMGs if os.path.basename(img_path) not in OUTLIERs]\n",
    "\n",
    "len(IMGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have vessel annotations for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_VESSELs = [\n",
    "    img_path for img_path in IMGs if is_medical_volume(img_path_to_vessel_path(img_path))\n",
    "]\n",
    "\n",
    "len(HAVE_VESSELs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of them. With a chosen ratio of 70-15-15, we select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_TEST = 0 # len(HAVE_VESSELs if VESSELs_REQUIRED else IMGs) * 15 // 100\n",
    "COUNT_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images to be part of the validation/testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case brain masks are available, we collect them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_BRAINs = [\n",
    "    img_path for img_path in IMGs if is_medical_volume(img_path_to_brain_path(img_path))\n",
    "]\n",
    "\n",
    "len(HAVE_BRAINs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we randomly split our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "VAL_IMGs = random.sample(sorted(HAVE_VESSELs), COUNT_TEST)\n",
    "\n",
    "print(\"val: \", [os.path.basename(img) for img in VAL_IMGs])\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "TEST_IMGs = random.sample(sorted([img for img in HAVE_VESSELs if img not in VAL_IMGs]), COUNT_TEST)\n",
    "\n",
    "print(\"test: \", [os.path.basename(img) for img in TEST_IMGs])\n",
    "\n",
    "TRAIN_IMGs = [img for img in (HAVE_VESSELs if VESSELs_REQUIRED else IMGs) if img not in VAL_IMGs and img not in TEST_IMGs]\n",
    "\n",
    "print(\"train: \", [os.path.basename(img) for img in TRAIN_IMGs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PUT THE FIRST 3 OF THE TRAINING SET INTO THE TEST SET (AND REMOVE THEM FROM THE TRAINING SET)\n",
    "# TEST_IMGs += TRAIN_IMGs[:3]\n",
    "# TRAIN_IMGs = TRAIN_IMGs[3:]\n",
    "\n",
    "# print(f\"train (LEN {len(TRAIN_IMGs)}): \", [os.path.basename(img) for img in TRAIN_IMGs])\n",
    "# print(f\"test: (LEN {len(TEST_IMGs)}): \", [os.path.basename(img) for img in TEST_IMGs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(f\"info_{DATASET_NAME}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        k: info[k].__dict__ if info[k] is not None else None for k in info\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract spacings and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_info_from_checkpoint\n",
    "\n",
    "info = load_info_from_checkpoint(f\"info_{DATASET_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_info_and_masks, loop_nii, display_info, get_target_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    \"train\": extract_info_and_masks(path_rule=PATH_RULE),\n",
    "    \"val\": extract_info_and_masks(path_rule=PATH_RULE),\n",
    "    \"test\": extract_info_and_masks(path_rule=PATH_RULE),\n",
    "}\n",
    "\n",
    "loop_nii(TRAIN_IMGs, info[\"train\"])\n",
    "loop_nii(VAL_IMGs, info[\"val\"])\n",
    "loop_nii(TEST_IMGs, info[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_info(info[\"train\"], info[\"train\"])#info[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACING = get_target_spacing(\n",
    "    info[\"train\"].spacings + info[\"val\"].spacings,\n",
    "    info[\"train\"].shapesAfterCropping() + info[\"val\"].shapesAfterCropping()\n",
    ")\n",
    "\n",
    "SPACING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"info_{DATASET_NAME}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        k: info[k].__dict__ if info[k] is not None else None for k in info\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop, Metadata, Resize, Empty Slices Removal, Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_info_from_checkpoint\n",
    "\n",
    "info = load_info_from_checkpoint(f\"info_{DATASET_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import print_bold, preprocessing_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bold(\"Training\")\n",
    "preprocessing_loop(\n",
    "    info[\"train\"],\n",
    "    os.path.join(OUTPUT_PATH,\"numpy\", \"train\"),\n",
    "    target_spacing=SPACING\n",
    ")\n",
    "\n",
    "# print_bold(\"Validation\")\n",
    "# preprocessing_loop(\n",
    "#     info[\"val\"],\n",
    "#     os.path.join(OUTPUT_PATH,\"numpy\", \"val\"),\n",
    "#     target_spacing=SPACING\n",
    "# )\n",
    "\n",
    "# print_bold(\"Testing\")\n",
    "# preprocessing_loop(\n",
    "#      info[\"test\"],\n",
    "#      os.path.join(OUTPUT_PATH,\"numpy\", \"test\"),\n",
    "#      target_spacing=SPACING\n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"info_{DATASET_NAME}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        k: info[k].__dict__ if info[k] is not None else None for k in info\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil \n",
    "\n",
    "def rename_dataset(DATASET_PATH_RENAMED, OUTPUT_PATH_RENAMED):\n",
    "    # DELETE OUTPUT FOLDER IF EXISTS\n",
    "    if os.path.exists(OUTPUT_PATH_RENAMED):\n",
    "        shutil.rmtree(OUTPUT_PATH_RENAMED)\n",
    "    \n",
    "    Path(OUTPUT_PATH_RENAMED).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Number of subjects: \", len(os.listdir(DATASET_PATH_RENAMED))/3)\n",
    "    os.listdir(DATASET_PATH_RENAMED)\n",
    "\n",
    "    dict_id = {}\n",
    "    counter = 0\n",
    "\n",
    "    for img in os.listdir(DATASET_PATH_RENAMED):\n",
    "        patient_id = img.split(\"_\")[0]\n",
    "        if patient_id not in dict_id:\n",
    "            counter += 1\n",
    "            dict_id[patient_id] = str(counter).zfill(3)\n",
    "        # copy to output folder and rename\\\n",
    "        print(\"Renaming \", img, \" to \", dict_id[patient_id] + \"_\" + img.split(\"_\")[-1])\n",
    "        shutil.copy(os.path.join(DATASET_PATH_RENAMED, img), os.path.join(OUTPUT_PATH_RENAMED, dict_id[patient_id] + \"_\" + img))\n",
    "        #os.rename(os.path.join(DATASET_PATH_RENAMED, img), os.path.join(OUTPUT_PATH_RENAMED, dict_id[patient_id] + \"_\" + img))\n",
    "\n",
    "rename_dataset(os.path.join(OUTPUT_PATH, \"numpy\", \"train\"), os.path.join(OUTPUT_PATH, \"numpy_renamed\", \"train\"))\n",
    "#rename_dataset(os.path.join(OUTPUT_PATH, \"numpy\", \"test\"), os.path.join(OUTPUT_PATH, \"numpy_renamed\", \"test\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
