{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"CAS\"\n",
    "DATA_PATH = \"/data/falcetta/brain_data/CAS/volumes\"\n",
    "OUT_PATH = \"/data/falcetta/brain_data/CASJ/preprocessed\"\n",
    "\n",
    "#these functions are to define for each new dataset, unless fixing a rule\n",
    "#they allow to define a clear association between the data path of a brain image and the corresponding brain/vessel mask\n",
    "#the brain mask is not mandatory, if not present just return None\n",
    "import utils\n",
    "def img_path_to_brain_path(img_path):\n",
    "    out_path =img_path.replace(\"CAS/volumes\",\"CAS/brain_masks_A2V\").replace(\".nii.gz\",\"_pred.nii.gz\")\n",
    "    return out_path\n",
    "def img_path_to_vessel_path(img_path):\n",
    "    out_path =img_path.replace(\"CAS/volumes/\",\"CAS/vessels/vessels_\")\n",
    "    return out_path\n",
    "def img_path_to_weight_path(img_path):\n",
    "    out_path =img_path.replace(\".nii.gz\",\"_weight.nii.gz\")\n",
    "    return out_path\n",
    "    \n",
    "utils.img_path_to_brain_path = img_path_to_brain_path\n",
    "utils.img_path_to_vessel_path = img_path_to_vessel_path\n",
    "utils.img_path_to_weight_path = img_path_to_weight_path\n",
    "\n",
    "#this regex works as a filter in case you have unnecessary .nii files in your dataset folder\n",
    "PATH_RULE = \"\" #\"^\\d{3}\\.nii\\.gz$\"\n",
    "\n",
    "\n",
    "#if you have some outliers you want to discard, you can specify it in the list below\n",
    "OUTLIERs = [\n",
    "    '068.nii.gz' # No brain mask\n",
    "]\n",
    "\n",
    "#if you need to preprocess also images with no vessel masks set the following flag to False\n",
    "VESSELs_REQUIRED = True\n",
    "\n",
    "#this parameter is to unify the orientation of our images\n",
    "#set it True if you notice in the examples shown below that the nose is oriented downward and not upward\n",
    "utils.do_flip = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import search_nii, extract_paths, load_info_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(f\"info_{DATASET_NAME}.pkl\"):\n",
    "    info = {\n",
    "        \"train\": extract_paths(path_rule=PATH_RULE),\n",
    "        \"val\": None,\n",
    "        \"test\": None,\n",
    "    }\n",
    "    search_nii(DATA_PATH, info[\"train\"])\n",
    "else:\n",
    "    info = load_info_from_checkpoint(f\"info_{DATASET_NAME}.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from utils import print_bold, load_and_display_middle_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_bold(\"[Example 01]\")\n",
    "random.seed(0)\n",
    "load_and_display_middle_slice(random.choice(info[\"train\"].paths), display_header=True)\n",
    "\n",
    "print_bold(\"[Example 02]\")\n",
    "random.seed(1)\n",
    "load_and_display_middle_slice(random.choice(info[\"train\"].paths), axis=[0,1,2], display_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import is_medical_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGs = sum([\n",
    "    info[\"train\"].paths,\n",
    "    info[\"val\"].paths if info[\"val\"] is not None else [],\n",
    "    info[\"test\"].paths if info[\"test\"] is not None else []\n",
    "], [])\n",
    "\n",
    "len(IMGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volumes. First, we discard the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGs = [img_path for img_path in IMGs if os.path.basename(img_path) not in OUTLIERs]\n",
    "\n",
    "#plot first 10 images PATHS from IMGs\n",
    "print(f\"Found {len(IMGs)} images\")\n",
    "print(f\"First 10 images:\")\n",
    "for img_path in IMGs[:10]:\n",
    "    print(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have vessel annotations for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_VESSELs = [\n",
    "    img_path for img_path in IMGs if is_medical_volume(img_path_to_vessel_path(img_path))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(HAVE_VESSELs)} images with vessel masks\")\n",
    "print(f\"First 10 images:\")\n",
    "for img_path in HAVE_VESSELs[:10]:\n",
    "    print(f\"Image: {img_path} - Vessel: {img_path_to_vessel_path(img_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of them. With a chosen ratio of 70-15-15, we select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_TEST = len(HAVE_VESSELs if VESSELs_REQUIRED else IMGs) * 16 // 100\n",
    "COUNT_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images to be part of the validation/testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case brain masks are available, we collect them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_BRAINs = [\n",
    "    img_path for img_path in IMGs if is_medical_volume(img_path_to_brain_path(img_path))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(HAVE_BRAINs)} images with brain masks\")\n",
    "print(f\"First 10 images:\")\n",
    "for img_path in HAVE_BRAINs[:10]:\n",
    "    print(f\"Image: {img_path} - Brain: {img_path_to_brain_path(img_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we randomly split our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "print(\"NO VAL SET ==> JUST TRAINING AND TEST SETS\")\n",
    "VAL_IMGs = [] #random.sample(sorted(HAVE_VESSELs), COUNT_TEST)\n",
    "\n",
    "print(\"val: \", [os.path.basename(img) for img in VAL_IMGs])\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "TEST_IMGs = random.sample(sorted([img for img in HAVE_VESSELs if img not in VAL_IMGs]), COUNT_TEST)\n",
    "\n",
    "print(\"test: \", [os.path.basename(img) for img in TEST_IMGs])\n",
    "\n",
    "TRAIN_IMGs = [img for img in (HAVE_VESSELs if VESSELs_REQUIRED else IMGs) if img not in VAL_IMGs and img not in TEST_IMGs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"info_{DATASET_NAME}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        k: info[k].__dict__ if info[k] is not None else None for k in info\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract spacings and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_info_from_checkpoint\n",
    "\n",
    "info = load_info_from_checkpoint(f\"info_{DATASET_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_info_and_masks, loop_nii, display_info, get_target_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "    \"train\": extract_info_and_masks(path_rule=PATH_RULE),\n",
    "    \"val\": extract_info_and_masks(path_rule=PATH_RULE),\n",
    "    \"test\": extract_info_and_masks(path_rule=PATH_RULE),\n",
    "}\n",
    "\n",
    "loop_nii(TRAIN_IMGs, info[\"train\"]) # ~1.30 min\n",
    "loop_nii(VAL_IMGs, info[\"val\"])\n",
    "loop_nii(TEST_IMGs, info[\"test\"]) # ~20 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_info(info[\"train\"], info[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACING = get_target_spacing(\n",
    "    info[\"train\"].spacings, #+ info[\"val\"].spacings,\n",
    "    info[\"train\"].shapesAfterCropping(),# + info[\"val\"].shapesAfterCropping()\n",
    ")\n",
    "\n",
    "SPACING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"info_{DATASET_NAME}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        k: info[k].__dict__ if info[k] is not None else None for k in info\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop, Metadata, Resize, Empty Slices Removal, Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_info_from_checkpoint\n",
    "\n",
    "info = load_info_from_checkpoint(f\"info_{DATASET_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_target_spacing\n",
    "\n",
    "SPACING = get_target_spacing(\n",
    "    info[\"train\"].spacings, #+ info[\"val\"].spacings,\n",
    "    info[\"train\"].shapesAfterCropping(),# + info[\"val\"].shapesAfterCropping()\n",
    ")\n",
    "\n",
    "SPACING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import print_bold, preprocessing_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bold(\"Training\")\n",
    "preprocessing_loop(\n",
    "    info[\"train\"],\n",
    "    os.path.join(OUT_PATH, \"numpy\", \"train\"),\n",
    "    target_spacing=SPACING,\n",
    "    discard_n_slices=5,\n",
    "    join_vessel_and_brain=True\n",
    ")\n",
    "\n",
    "# print_bold(\"Validation\")\n",
    "# preprocessing_loop(\n",
    "#     info[\"val\"],\n",
    "#     os.path.join(OUT_PATH, \"numpy\", \"val\"),\n",
    "#     target_spacing=SPACING\n",
    "# )\n",
    "\n",
    "print_bold(\"Testing\")\n",
    "preprocessing_loop(\n",
    "    info[\"test\"],\n",
    "    os.path.join(OUT_PATH, \"numpy\", \"test\"),\n",
    "    target_spacing=SPACING,\n",
    "    discard_n_slices=5,\n",
    "    join_vessel_and_brain=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"info_{DATASET_NAME}.pkl\", \"wb\") as file:\n",
    "    pickle.dump({\n",
    "        k: info[k].__dict__ if info[k] is not None else None for k in info\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
